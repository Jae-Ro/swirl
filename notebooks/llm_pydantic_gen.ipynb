{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ebfeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaero/projects/interviews/team_raft/dq-swirl/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from typing import List, Optional\n",
    "\n",
    "import tqdm as notebook_tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, constr, field_validator\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import logging as transformers_logging\n",
    "\n",
    "from dq_swirl.clients.async_llm_client import AsyncLLMClient\n",
    "from dq_swirl.ingestion.structure_analyzer import StructuralAnalyzer\n",
    "from dq_swirl.rust_ingestion import smart_parse_batch\n",
    "\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6168d974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"../secrets.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067ba83",
   "metadata": {},
   "source": [
    "## Messy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9549bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_data = [\n",
    "    \"Order 1001: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\",\n",
    "    \"Order 1004:   Buyer=  AMANDA SMITH ,Location=Seattle, WA,Total=$50.00, Items: desk lamp\",\n",
    "    \"Order 1005: Buyer=Raj Patel, Total=1,200.50, Items: monitor, stand, cable\",\n",
    "    \"Order 1006: total=$89.99, location=Miami, FL, buyer=Elena Rossi, Items: keyboard\",\n",
    "    \"Order 1007: Buyer=Chris P., Location=Denver, CO, Total=$12.00, Items: stickers -- [DISCOUNT APPLIED]\",\n",
    "    \"Order 1008: Buyer=O'Connor, S., Location=Portland, OR, Total=$0.00, Items: \",\n",
    "    \"Order 1011: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\",\n",
    "    \"Order 1012: Buyer=Sarah Liu, Location=Austin, TX, Total=$156.55, Items: headphones\",\n",
    "    \"Order 1013: Buyer=Mike Turner, Location=Cleveland, OH, Total=$1299.99, Items: gaming pc, mouse\",\n",
    "    \"Order 1014: Buyer=Rachel Kim, Locadtion=Seattle, WA, Total=$89.50, Items: coffee maker\",\n",
    "    \"Order 1015: Buyer=Chris Myers, Location=Cincinnati, OH, Total=$512.00, Items: monitor, desk lamp\",\n",
    "    \"Order=1016, Buyer=Jake Myers, Total=$1,512.00, Items: monitor,\",\n",
    "    '{\"id\": \"usr_001\", \"name\": \"Alex Johnson\", \"role\": \"admin\", \"isActive\": true, \"createdAt\": \"2025-11-02T09:14:23Z\"}',\n",
    "    '{\"id\": \"usr_002\", \"name\": \"Maria Lopez\", \"email\": \"maria.lopez@example.com\", \"role\": \"editor\", \"isActive\": null, \"createdAt\": \"2025-12-18T16:47:10Z\", \"lastLoginIp\": \"192.168.1.42\"}',\n",
    "    '{\"id\": \"usr_003\", \"email\": \"samir.patel@example.com\", \"role\": \"viewer\", \"isActive\": false, \"createdAt\": \"08/05/2024\"}',\n",
    "    '{\"id\": 4, \"name\": \"Chen Wei\", \"email\": \"chen.wei@example.com\", \"isActive\": true, \"createdAt\": null}',\n",
    "    '{\"id\": \"usr_005\", \"name\": \"Broken Record\", \"email\": \"broken@example.com\"}',\n",
    "    \"Order 1017: Buyer=Griffin Arora, Location=Columbia, SC, Total=$512.00, Items: monitor, desk lamp, Discount: yes\",\n",
    "    # \"Order=1018, Buyer=Jae Arora, Location=Dreher, FL, Total=$6.00, Items: chair, Discount: true, phone=123-456-789\" ,\n",
    "    # \"Order=1019, Buyer=Jae Kao, Location=Atlanta, GA, Total=$12.00, Items: desk, Discount: False, phone=123-456-789\" ,\n",
    "    \"2026-01-30 14:22:01 INFO User login successful user_id=123\",\n",
    "    \"2026-01-30 14:22:01 INFO User login successful\",\n",
    "    \"level =INFO, user =Sam, id=1\",\n",
    "    \"timestamp=2026-01-30T14:22:01Z level=INFO user=alice action=login success=true\",\n",
    "    \"level=INFO cpu_usage=1,234.56 memory=512MB\",\n",
    "    '{\"level\":\"INFO\",\"service\":\"orders\",\"order_id\":1001,\"status\":\"created\"}',\n",
    "    '[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"',\n",
    "    \"2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d865678",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be705a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UNSTRUCTURED STRING SAMPLES: 20\n",
      "\n",
      "JSON STRING SAMPLES: 6\n",
      "\n",
      "Original: Order 1001: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\n",
      "Parsed: {'order': '1001', 'buyer': 'John Davis', 'location': 'Columbus, OH', 'total': '$742.10', 'items': 'laptop, hdmi cable'}\n",
      "\n",
      "Original: Order 1004:   Buyer=  AMANDA SMITH ,Location=Seattle, WA,Total=$50.00, Items: desk lamp\n",
      "Parsed: {'order': '1004', 'buyer': 'AMANDA SMITH', 'location': 'Seattle, WA', 'total': '$50.00', 'items': 'desk lamp'}\n",
      "\n",
      "Original: Order 1005: Buyer=Raj Patel, Total=1,200.50, Items: monitor, stand, cable\n",
      "Parsed: {'order': '1005', 'buyer': 'Raj Patel', 'total': '1,200.50', 'items': 'monitor, stand, cable'}\n",
      "\n",
      "Original: Order 1006: total=$89.99, location=Miami, FL, buyer=Elena Rossi, Items: keyboard\n",
      "Parsed: {'order': '1006', 'total': '$89.99', 'location': 'Miami, FL', 'buyer': 'Elena Rossi', 'items': 'keyboard'}\n",
      "\n",
      "Original: Order 1007: Buyer=Chris P., Location=Denver, CO, Total=$12.00, Items: stickers -- [DISCOUNT APPLIED]\n",
      "Parsed: {'order': '1007', 'buyer': 'Chris P.', 'location': 'Denver, CO', 'total': '$12.00', 'items': 'stickers -- [DISCOUNT APPLIED]'}\n",
      "\n",
      "Original: Order 1008: Buyer=O'Connor, S., Location=Portland, OR, Total=$0.00, Items: \n",
      "Parsed: {'order': '1008', 'buyer': \"O'Connor, S.\", 'location': 'Portland, OR', 'total': '$0.00', 'items': 'None'}\n",
      "\n",
      "Original: Order 1011: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\n",
      "Parsed: {'order': '1011', 'buyer': 'John Davis', 'location': 'Columbus, OH', 'total': '$742.10', 'items': 'laptop, hdmi cable'}\n",
      "\n",
      "Original: Order 1012: Buyer=Sarah Liu, Location=Austin, TX, Total=$156.55, Items: headphones\n",
      "Parsed: {'order': '1012', 'buyer': 'Sarah Liu', 'location': 'Austin, TX', 'total': '$156.55', 'items': 'headphones'}\n",
      "\n",
      "Original: Order 1013: Buyer=Mike Turner, Location=Cleveland, OH, Total=$1299.99, Items: gaming pc, mouse\n",
      "Parsed: {'order': '1013', 'buyer': 'Mike Turner', 'location': 'Cleveland, OH', 'total': '$1299.99', 'items': 'gaming pc, mouse'}\n",
      "\n",
      "Original: Order 1014: Buyer=Rachel Kim, Locadtion=Seattle, WA, Total=$89.50, Items: coffee maker\n",
      "Parsed: {'order': '1014', 'buyer': 'Rachel Kim', 'locadtion': 'Seattle, WA', 'total': '$89.50', 'items': 'coffee maker'}\n",
      "\n",
      "Original: Order 1015: Buyer=Chris Myers, Location=Cincinnati, OH, Total=$512.00, Items: monitor, desk lamp\n",
      "Parsed: {'order': '1015', 'buyer': 'Chris Myers', 'location': 'Cincinnati, OH', 'total': '$512.00', 'items': 'monitor, desk lamp'}\n",
      "\n",
      "Original: Order=1016, Buyer=Jake Myers, Total=$1,512.00, Items: monitor,\n",
      "Parsed: {'order': '1016', 'buyer': 'Jake Myers', 'total': '$1,512.00', 'items': 'monitor'}\n",
      "\n",
      "Original: Order 1017: Buyer=Griffin Arora, Location=Columbia, SC, Total=$512.00, Items: monitor, desk lamp, Discount: yes\n",
      "Parsed: {'order': '1017', 'buyer': 'Griffin Arora', 'location': 'Columbia, SC', 'total': '$512.00', 'items': 'monitor, desk lamp', 'discount': 'yes'}\n",
      "\n",
      "Original: 2026-01-30 14:22:01 INFO User login successful user_id=123\n",
      "Parsed: {'user_id': '123', '_unparsed': '2026-01-30 14:22:01 INFO User login successful'}\n",
      "\n",
      "Original: 2026-01-30 14:22:01 INFO User login successful\n",
      "Parsed: {'_unparsed': '2026-01-30 14:22:01 INFO User login successful'}\n",
      "\n",
      "Original: level =INFO, user =Sam, id=1\n",
      "Parsed: {'level': 'INFO', 'user': 'Sam', 'id': '1'}\n",
      "\n",
      "Original: timestamp=2026-01-30T14:22:01Z level=INFO user=alice action=login success=true\n",
      "Parsed: {'timestamp': '2026-01-30T14:22:01Z', 'level': 'INFO', 'user': 'alice', 'action': 'login', 'success': 'true'}\n",
      "\n",
      "Original: level=INFO cpu_usage=1,234.56 memory=512MB\n",
      "Parsed: {'level': 'INFO', 'cpu_usage': '1,234.56', 'memory': '512MB'}\n",
      "\n",
      "Original: [2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"\n",
      "Parsed: {'_unparsed': '[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"'}\n",
      "\n",
      "Original: 2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check\n",
      "Parsed: {'_unparsed': '2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check'}\n",
      "\n",
      "\n",
      "TOTAL SAMPLES: 26\n",
      "ERROR SAMPLES: 0\n",
      "\n",
      "Detected 15 unique schemas across 26 records.\n",
      "\n",
      "Schema 28d9f3b14d0e5516a186062212502d0c (1 occurrences):\n",
      "  Layout: {'order': 'str', 'buyer': 'str', 'locadtion': 'str', 'total': 'str', 'items': 'str'}\n",
      "------------------------------\n",
      "Schema 2bcfb738a2056c5ef5543917c9f3ac53 (1 occurrences):\n",
      "  Layout: {'id': 'int', 'name': 'str', 'email': 'str', 'isactive': 'bool', 'createdat': 'NoneType'}\n",
      "------------------------------\n",
      "Schema 3baae1f59cac077e89e2f0b7d47a36cf (1 occurrences):\n",
      "  Layout: {'user_id': 'str', '_unparsed': 'str'}\n",
      "------------------------------\n",
      "Schema 4286ed8928a42e13682233f00079d106 (1 occurrences):\n",
      "  Layout: {'id': 'str', 'name': 'str', 'email': 'str', 'role': 'str', 'isactive': 'NoneType', 'createdat': 'str', 'lastloginip': 'str'}\n",
      "------------------------------\n",
      "Schema 461a895ef9c5046dd2cb5026b6a62de0 (1 occurrences):\n",
      "  Layout: {'order': 'str', 'buyer': 'str', 'location': 'str', 'total': 'str', 'items': 'str', 'discount': 'str'}\n",
      "------------------------------\n",
      "Schema 50eb97a85647221ecc7f65f74d68d156 (2 occurrences):\n",
      "  Layout: {'order': 'str', 'buyer': 'str', 'total': 'str', 'items': 'str'}\n",
      "------------------------------\n",
      "Schema 74db6b18a3e440d4dd054614492fa510 (1 occurrences):\n",
      "  Layout: {'timestamp': 'str', 'level': 'str', 'user': 'str', 'action': 'str', 'success': 'str'}\n",
      "------------------------------\n",
      "Schema a0713c61038426204fef9da85c91686f (1 occurrences):\n",
      "  Layout: {'id': 'str', 'name': 'str', 'role': 'str', 'isactive': 'bool', 'createdat': 'str'}\n",
      "------------------------------\n",
      "Schema c2aebc126ff252eb9dae084d659d2cab (1 occurrences):\n",
      "  Layout: {'level': 'str', 'user': 'str', 'id': 'str'}\n",
      "------------------------------\n",
      "Schema d0f1c5f0782666cd8324b4cf57a3159d (1 occurrences):\n",
      "  Layout: {'level': 'str', 'cpu_usage': 'str', 'memory': 'str'}\n",
      "------------------------------\n",
      "Schema d2d16f7c3698c6195ddaeb6205139150 (1 occurrences):\n",
      "  Layout: {'id': 'str', 'name': 'str', 'email': 'str'}\n",
      "------------------------------\n",
      "Schema d5d811706d95e2f6c1b713aa3fd1d9ab (1 occurrences):\n",
      "  Layout: {'id': 'str', 'email': 'str', 'role': 'str', 'isactive': 'bool', 'createdat': 'str'}\n",
      "------------------------------\n",
      "Schema df87355cb94200d773396f5befa867d4 (3 occurrences):\n",
      "  Layout: {'_unparsed': 'str'}\n",
      "------------------------------\n",
      "Schema e8b13b0b419896ad0f402c15a762e2c1 (1 occurrences):\n",
      "  Layout: {'level': 'str', 'service': 'str', 'order_id': 'int', 'status': 'str'}\n",
      "------------------------------\n",
      "Schema fd116cd512d5ecd2e59edf12fc258b32 (9 occurrences):\n",
      "  Layout: {'order': 'str', 'buyer': 'str', 'location': 'str', 'total': 'str', 'items': 'str'}\n",
      "------------------------------\n",
      "Structural Clusters: \n",
      "{\n",
      "    \"1\": [\n",
      "        {\n",
      "            \"signature_hash\": \"28d9f3b14d0e5516a186062212502d0c\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"locadtion\",\n",
      "                \"total\",\n",
      "                \"items\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"461a895ef9c5046dd2cb5026b6a62de0\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"location\",\n",
      "                \"total\",\n",
      "                \"items\",\n",
      "                \"discount\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"50eb97a85647221ecc7f65f74d68d156\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"total\",\n",
      "                \"items\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"fd116cd512d5ecd2e59edf12fc258b32\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"location\",\n",
      "                \"total\",\n",
      "                \"items\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ],\n",
      "    \"0\": [\n",
      "        {\n",
      "            \"signature_hash\": \"2bcfb738a2056c5ef5543917c9f3ac53\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"email\",\n",
      "                \"isactive\",\n",
      "                \"createdat\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"4286ed8928a42e13682233f00079d106\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"email\",\n",
      "                \"role\",\n",
      "                \"isactive\",\n",
      "                \"createdat\",\n",
      "                \"lastloginip\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"a0713c61038426204fef9da85c91686f\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"role\",\n",
      "                \"isactive\",\n",
      "                \"createdat\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"d2d16f7c3698c6195ddaeb6205139150\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"email\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"d5d811706d95e2f6c1b713aa3fd1d9ab\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"email\",\n",
      "                \"role\",\n",
      "                \"isactive\",\n",
      "                \"createdat\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ],\n",
      "    \"3\": [\n",
      "        {\n",
      "            \"signature_hash\": \"3baae1f59cac077e89e2f0b7d47a36cf\",\n",
      "            \"fields\": [\n",
      "                \"user_id\",\n",
      "                \"_unparsed\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"df87355cb94200d773396f5befa867d4\",\n",
      "            \"fields\": [\n",
      "                \"_unparsed\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"e8b13b0b419896ad0f402c15a762e2c1\",\n",
      "            \"fields\": [\n",
      "                \"level\",\n",
      "                \"service\",\n",
      "                \"order_id\",\n",
      "                \"status\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ],\n",
      "    \"2\": [\n",
      "        {\n",
      "            \"signature_hash\": \"74db6b18a3e440d4dd054614492fa510\",\n",
      "            \"fields\": [\n",
      "                \"timestamp\",\n",
      "                \"level\",\n",
      "                \"user\",\n",
      "                \"action\",\n",
      "                \"success\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"c2aebc126ff252eb9dae084d659d2cab\",\n",
      "            \"fields\": [\n",
      "                \"level\",\n",
      "                \"user\",\n",
      "                \"id\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"d0f1c5f0782666cd8324b4cf57a3159d\",\n",
      "            \"fields\": [\n",
      "                \"level\",\n",
      "                \"cpu_usage\",\n",
      "                \"memory\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "{\n",
      "    \"fd116cd512d5ecd2e59edf12fc258b32\": {\n",
      "        \"signature\": {\n",
      "            \"order\": \"str\",\n",
      "            \"buyer\": \"str\",\n",
      "            \"location\": \"str\",\n",
      "            \"total\": \"str\",\n",
      "            \"items\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"Order 1001: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1001\",\n",
      "                    \"buyer\": \"John Davis\",\n",
      "                    \"location\": \"Columbus, OH\",\n",
      "                    \"total\": \"$742.10\",\n",
      "                    \"items\": \"laptop, hdmi cable\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1004:   Buyer=  AMANDA SMITH ,Location=Seattle, WA,Total=$50.00, Items: desk lamp\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1004\",\n",
      "                    \"buyer\": \"AMANDA SMITH\",\n",
      "                    \"location\": \"Seattle, WA\",\n",
      "                    \"total\": \"$50.00\",\n",
      "                    \"items\": \"desk lamp\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1006: total=$89.99, location=Miami, FL, buyer=Elena Rossi, Items: keyboard\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1006\",\n",
      "                    \"total\": \"$89.99\",\n",
      "                    \"location\": \"Miami, FL\",\n",
      "                    \"buyer\": \"Elena Rossi\",\n",
      "                    \"items\": \"keyboard\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1007: Buyer=Chris P., Location=Denver, CO, Total=$12.00, Items: stickers -- [DISCOUNT APPLIED]\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1007\",\n",
      "                    \"buyer\": \"Chris P.\",\n",
      "                    \"location\": \"Denver, CO\",\n",
      "                    \"total\": \"$12.00\",\n",
      "                    \"items\": \"stickers -- [DISCOUNT APPLIED]\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1008: Buyer=O'Connor, S., Location=Portland, OR, Total=$0.00, Items: \",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1008\",\n",
      "                    \"buyer\": \"O'Connor, S.\",\n",
      "                    \"location\": \"Portland, OR\",\n",
      "                    \"total\": \"$0.00\",\n",
      "                    \"items\": \"None\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1011: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1011\",\n",
      "                    \"buyer\": \"John Davis\",\n",
      "                    \"location\": \"Columbus, OH\",\n",
      "                    \"total\": \"$742.10\",\n",
      "                    \"items\": \"laptop, hdmi cable\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1012: Buyer=Sarah Liu, Location=Austin, TX, Total=$156.55, Items: headphones\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1012\",\n",
      "                    \"buyer\": \"Sarah Liu\",\n",
      "                    \"location\": \"Austin, TX\",\n",
      "                    \"total\": \"$156.55\",\n",
      "                    \"items\": \"headphones\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1013: Buyer=Mike Turner, Location=Cleveland, OH, Total=$1299.99, Items: gaming pc, mouse\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1013\",\n",
      "                    \"buyer\": \"Mike Turner\",\n",
      "                    \"location\": \"Cleveland, OH\",\n",
      "                    \"total\": \"$1299.99\",\n",
      "                    \"items\": \"gaming pc, mouse\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1015: Buyer=Chris Myers, Location=Cincinnati, OH, Total=$512.00, Items: monitor, desk lamp\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1015\",\n",
      "                    \"buyer\": \"Chris Myers\",\n",
      "                    \"location\": \"Cincinnati, OH\",\n",
      "                    \"total\": \"$512.00\",\n",
      "                    \"items\": \"monitor, desk lamp\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"50eb97a85647221ecc7f65f74d68d156\": {\n",
      "        \"signature\": {\n",
      "            \"order\": \"str\",\n",
      "            \"buyer\": \"str\",\n",
      "            \"total\": \"str\",\n",
      "            \"items\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"Order 1005: Buyer=Raj Patel, Total=1,200.50, Items: monitor, stand, cable\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1005\",\n",
      "                    \"buyer\": \"Raj Patel\",\n",
      "                    \"total\": \"1,200.50\",\n",
      "                    \"items\": \"monitor, stand, cable\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order=1016, Buyer=Jake Myers, Total=$1,512.00, Items: monitor,\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1016\",\n",
      "                    \"buyer\": \"Jake Myers\",\n",
      "                    \"total\": \"$1,512.00\",\n",
      "                    \"items\": \"monitor\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"28d9f3b14d0e5516a186062212502d0c\": {\n",
      "        \"signature\": {\n",
      "            \"order\": \"str\",\n",
      "            \"buyer\": \"str\",\n",
      "            \"locadtion\": \"str\",\n",
      "            \"total\": \"str\",\n",
      "            \"items\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"Order 1014: Buyer=Rachel Kim, Locadtion=Seattle, WA, Total=$89.50, Items: coffee maker\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1014\",\n",
      "                    \"buyer\": \"Rachel Kim\",\n",
      "                    \"locadtion\": \"Seattle, WA\",\n",
      "                    \"total\": \"$89.50\",\n",
      "                    \"items\": \"coffee maker\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"461a895ef9c5046dd2cb5026b6a62de0\": {\n",
      "        \"signature\": {\n",
      "            \"order\": \"str\",\n",
      "            \"buyer\": \"str\",\n",
      "            \"location\": \"str\",\n",
      "            \"total\": \"str\",\n",
      "            \"items\": \"str\",\n",
      "            \"discount\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"Order 1017: Buyer=Griffin Arora, Location=Columbia, SC, Total=$512.00, Items: monitor, desk lamp, Discount: yes\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1017\",\n",
      "                    \"buyer\": \"Griffin Arora\",\n",
      "                    \"location\": \"Columbia, SC\",\n",
      "                    \"total\": \"$512.00\",\n",
      "                    \"items\": \"monitor, desk lamp\",\n",
      "                    \"discount\": \"yes\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"3baae1f59cac077e89e2f0b7d47a36cf\": {\n",
      "        \"signature\": {\n",
      "            \"user_id\": \"str\",\n",
      "            \"_unparsed\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"2026-01-30 14:22:01 INFO User login successful user_id=123\",\n",
      "                \"parsed\": {\n",
      "                    \"user_id\": \"123\",\n",
      "                    \"_unparsed\": \"2026-01-30 14:22:01 INFO User login successful\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"df87355cb94200d773396f5befa867d4\": {\n",
      "        \"signature\": {\n",
      "            \"_unparsed\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"2026-01-30 14:22:01 INFO User login successful\",\n",
      "                \"parsed\": {\n",
      "                    \"_unparsed\": \"2026-01-30 14:22:01 INFO User login successful\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \\\"GET /health 1.1\\\" 200 16 \\\"-\\\" \\\"curl/8.14.1\\\"\",\n",
      "                \"parsed\": {\n",
      "                    \"_unparsed\": \"[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \\\"GET /health 1.1\\\" 200 16 \\\"-\\\" \\\"curl/8.14.1\\\"\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check\",\n",
      "                \"parsed\": {\n",
      "                    \"_unparsed\": \"2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"c2aebc126ff252eb9dae084d659d2cab\": {\n",
      "        \"signature\": {\n",
      "            \"level\": \"str\",\n",
      "            \"user\": \"str\",\n",
      "            \"id\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"level =INFO, user =Sam, id=1\",\n",
      "                \"parsed\": {\n",
      "                    \"level\": \"INFO\",\n",
      "                    \"user\": \"Sam\",\n",
      "                    \"id\": \"1\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"74db6b18a3e440d4dd054614492fa510\": {\n",
      "        \"signature\": {\n",
      "            \"timestamp\": \"str\",\n",
      "            \"level\": \"str\",\n",
      "            \"user\": \"str\",\n",
      "            \"action\": \"str\",\n",
      "            \"success\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"timestamp=2026-01-30T14:22:01Z level=INFO user=alice action=login success=true\",\n",
      "                \"parsed\": {\n",
      "                    \"timestamp\": \"2026-01-30T14:22:01Z\",\n",
      "                    \"level\": \"INFO\",\n",
      "                    \"user\": \"alice\",\n",
      "                    \"action\": \"login\",\n",
      "                    \"success\": \"true\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"d0f1c5f0782666cd8324b4cf57a3159d\": {\n",
      "        \"signature\": {\n",
      "            \"level\": \"str\",\n",
      "            \"cpu_usage\": \"str\",\n",
      "            \"memory\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"level=INFO cpu_usage=1,234.56 memory=512MB\",\n",
      "                \"parsed\": {\n",
      "                    \"level\": \"INFO\",\n",
      "                    \"cpu_usage\": \"1,234.56\",\n",
      "                    \"memory\": \"512MB\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"a0713c61038426204fef9da85c91686f\": {\n",
      "        \"signature\": {\n",
      "            \"id\": \"str\",\n",
      "            \"name\": \"str\",\n",
      "            \"role\": \"str\",\n",
      "            \"isactive\": \"bool\",\n",
      "            \"createdat\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"{\\\"id\\\": \\\"usr_001\\\", \\\"name\\\": \\\"Alex Johnson\\\", \\\"role\\\": \\\"admin\\\", \\\"isActive\\\": true, \\\"createdAt\\\": \\\"2025-11-02T09:14:23Z\\\"}\",\n",
      "                \"parsed\": {\n",
      "                    \"id\": \"usr_001\",\n",
      "                    \"name\": \"Alex Johnson\",\n",
      "                    \"role\": \"admin\",\n",
      "                    \"isActive\": true,\n",
      "                    \"createdAt\": \"2025-11-02T09:14:23Z\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"4286ed8928a42e13682233f00079d106\": {\n",
      "        \"signature\": {\n",
      "            \"id\": \"str\",\n",
      "            \"name\": \"str\",\n",
      "            \"email\": \"str\",\n",
      "            \"role\": \"str\",\n",
      "            \"isactive\": \"NoneType\",\n",
      "            \"createdat\": \"str\",\n",
      "            \"lastloginip\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"{\\\"id\\\": \\\"usr_002\\\", \\\"name\\\": \\\"Maria Lopez\\\", \\\"email\\\": \\\"maria.lopez@example.com\\\", \\\"role\\\": \\\"editor\\\", \\\"isActive\\\": null, \\\"createdAt\\\": \\\"2025-12-18T16:47:10Z\\\", \\\"lastLoginIp\\\": \\\"192.168.1.42\\\"}\",\n",
      "                \"parsed\": {\n",
      "                    \"id\": \"usr_002\",\n",
      "                    \"name\": \"Maria Lopez\",\n",
      "                    \"email\": \"maria.lopez@example.com\",\n",
      "                    \"role\": \"editor\",\n",
      "                    \"isActive\": null,\n",
      "                    \"createdAt\": \"2025-12-18T16:47:10Z\",\n",
      "                    \"lastLoginIp\": \"192.168.1.42\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"d5d811706d95e2f6c1b713aa3fd1d9ab\": {\n",
      "        \"signature\": {\n",
      "            \"id\": \"str\",\n",
      "            \"email\": \"str\",\n",
      "            \"role\": \"str\",\n",
      "            \"isactive\": \"bool\",\n",
      "            \"createdat\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"{\\\"id\\\": \\\"usr_003\\\", \\\"email\\\": \\\"samir.patel@example.com\\\", \\\"role\\\": \\\"viewer\\\", \\\"isActive\\\": false, \\\"createdAt\\\": \\\"08/05/2024\\\"}\",\n",
      "                \"parsed\": {\n",
      "                    \"id\": \"usr_003\",\n",
      "                    \"email\": \"samir.patel@example.com\",\n",
      "                    \"role\": \"viewer\",\n",
      "                    \"isActive\": false,\n",
      "                    \"createdAt\": \"08/05/2024\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"2bcfb738a2056c5ef5543917c9f3ac53\": {\n",
      "        \"signature\": {\n",
      "            \"id\": \"int\",\n",
      "            \"name\": \"str\",\n",
      "            \"email\": \"str\",\n",
      "            \"isactive\": \"bool\",\n",
      "            \"createdat\": \"NoneType\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"{\\\"id\\\": 4, \\\"name\\\": \\\"Chen Wei\\\", \\\"email\\\": \\\"chen.wei@example.com\\\", \\\"isActive\\\": true, \\\"createdAt\\\": null}\",\n",
      "                \"parsed\": {\n",
      "                    \"id\": 4,\n",
      "                    \"name\": \"Chen Wei\",\n",
      "                    \"email\": \"chen.wei@example.com\",\n",
      "                    \"isActive\": true,\n",
      "                    \"createdAt\": null\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"d2d16f7c3698c6195ddaeb6205139150\": {\n",
      "        \"signature\": {\n",
      "            \"id\": \"str\",\n",
      "            \"name\": \"str\",\n",
      "            \"email\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"{\\\"id\\\": \\\"usr_005\\\", \\\"name\\\": \\\"Broken Record\\\", \\\"email\\\": \\\"broken@example.com\\\"}\",\n",
      "                \"parsed\": {\n",
      "                    \"id\": \"usr_005\",\n",
      "                    \"name\": \"Broken Record\",\n",
      "                    \"email\": \"broken@example.com\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"e8b13b0b419896ad0f402c15a762e2c1\": {\n",
      "        \"signature\": {\n",
      "            \"level\": \"str\",\n",
      "            \"service\": \"str\",\n",
      "            \"order_id\": \"int\",\n",
      "            \"status\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"{\\\"level\\\":\\\"INFO\\\",\\\"service\\\":\\\"orders\\\",\\\"order_id\\\":1001,\\\"status\\\":\\\"created\\\"}\",\n",
      "                \"parsed\": {\n",
      "                    \"level\": \"INFO\",\n",
      "                    \"service\": \"orders\",\n",
      "                    \"order_id\": 1001,\n",
      "                    \"status\": \"created\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1945.13it/s, Materializing param=pooler.dense.weight]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Clusters: \n",
      "{\n",
      "    \"0\": [\n",
      "        {\n",
      "            \"signature_hash\": \"28d9f3b14d0e5516a186062212502d0c\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"locadtion\",\n",
      "                \"total\",\n",
      "                \"items\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"461a895ef9c5046dd2cb5026b6a62de0\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"location\",\n",
      "                \"total\",\n",
      "                \"items\",\n",
      "                \"discount\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"50eb97a85647221ecc7f65f74d68d156\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"total\",\n",
      "                \"items\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"fd116cd512d5ecd2e59edf12fc258b32\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"location\",\n",
      "                \"total\",\n",
      "                \"items\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ],\n",
      "    \"2\": [\n",
      "        {\n",
      "            \"signature_hash\": \"2bcfb738a2056c5ef5543917c9f3ac53\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"email\",\n",
      "                \"isactive\",\n",
      "                \"createdat\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"4286ed8928a42e13682233f00079d106\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"email\",\n",
      "                \"role\",\n",
      "                \"isactive\",\n",
      "                \"createdat\",\n",
      "                \"lastloginip\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"a0713c61038426204fef9da85c91686f\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"role\",\n",
      "                \"isactive\",\n",
      "                \"createdat\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"d2d16f7c3698c6195ddaeb6205139150\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"email\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"d5d811706d95e2f6c1b713aa3fd1d9ab\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"email\",\n",
      "                \"role\",\n",
      "                \"isactive\",\n",
      "                \"createdat\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ],\n",
      "    \"1\": [\n",
      "        {\n",
      "            \"signature_hash\": \"3baae1f59cac077e89e2f0b7d47a36cf\",\n",
      "            \"fields\": [\n",
      "                \"user_id\",\n",
      "                \"_unparsed\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"c2aebc126ff252eb9dae084d659d2cab\",\n",
      "            \"fields\": [\n",
      "                \"level\",\n",
      "                \"user\",\n",
      "                \"id\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ],\n",
      "    \"-1\": [\n",
      "        {\n",
      "            \"signature_hash\": \"74db6b18a3e440d4dd054614492fa510\",\n",
      "            \"fields\": [\n",
      "                \"timestamp\",\n",
      "                \"level\",\n",
      "                \"user\",\n",
      "                \"action\",\n",
      "                \"success\"\n",
      "            ],\n",
      "            \"is_outlier\": true\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"d0f1c5f0782666cd8324b4cf57a3159d\",\n",
      "            \"fields\": [\n",
      "                \"level\",\n",
      "                \"cpu_usage\",\n",
      "                \"memory\"\n",
      "            ],\n",
      "            \"is_outlier\": true\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"df87355cb94200d773396f5befa867d4\",\n",
      "            \"fields\": [\n",
      "                \"_unparsed\"\n",
      "            ],\n",
      "            \"is_outlier\": true\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"e8b13b0b419896ad0f402c15a762e2c1\",\n",
      "            \"fields\": [\n",
      "                \"level\",\n",
      "                \"service\",\n",
      "                \"order_id\",\n",
      "                \"status\"\n",
      "            ],\n",
      "            \"is_outlier\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "################################# Grammar Parsing ###############################\n",
    "#################################################################################\n",
    "\n",
    "string_batch = []\n",
    "string_json_batch = []\n",
    "for msg in messy_data:\n",
    "    if not (msg.startswith(\"[\") and msg.endswith(\"]\")) and not (\n",
    "        msg.startswith(\"{\") and msg.endswith(\"}\")\n",
    "    ):\n",
    "        string_batch.append(msg)\n",
    "    else:\n",
    "        string_json_batch.append(msg)\n",
    "\n",
    "print(f\"\\nUNSTRUCTURED STRING SAMPLES: {len(string_batch)}\\n\")\n",
    "print(f\"JSON STRING SAMPLES: {len(string_json_batch)}\\n\")\n",
    "\n",
    "\n",
    "string_samples = smart_parse_batch(string_batch)\n",
    "\n",
    "for i, (msg, parsed) in enumerate(string_samples):\n",
    "    print(f\"Original: {msg}\\nParsed: {parsed}\\n\")\n",
    "\n",
    "\n",
    "json_samples = []\n",
    "leftovers = []\n",
    "\n",
    "for msg in string_json_batch:\n",
    "    try:\n",
    "        data = json.loads(msg)\n",
    "        json_samples.append((msg, data))\n",
    "    except Exception:\n",
    "        leftovers.append((msg, None))\n",
    "\n",
    "\n",
    "data_samples = string_samples + json_samples\n",
    "\n",
    "print(f\"\\nTOTAL SAMPLES: {len(data_samples)}\\nERROR SAMPLES: {len(leftovers)}\\n\")\n",
    "\n",
    "#################################################################################\n",
    "############################### Structure Analyzer ##############################\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "analyzer = StructuralAnalyzer(ignore_unparsed=False)\n",
    "\n",
    "hash_counts = Counter()\n",
    "unique_structures = {}\n",
    "\n",
    "for raw, parsed in data_samples:\n",
    "    result = analyzer.generate_fingerprint(raw, parsed)\n",
    "    signature_hash = result[\"hash\"]\n",
    "    hash_counts[signature_hash] += 1\n",
    "    unique_structures[signature_hash] = unique_structures.get(signature_hash, result)\n",
    "\n",
    "print(\n",
    "    f\"Detected {len(unique_structures)} unique schemas across {len(data_samples)} records.\\n\"\n",
    ")\n",
    "\n",
    "for h, count in sorted(hash_counts.items()):\n",
    "    print(f\"Schema {h} ({count} occurrences):\")\n",
    "    print(f\"  Layout: {unique_structures[h]['signature']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "############################# Structural Clustering #############################\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "def conjoin_signatures(registry_output: dict):\n",
    "    hashes = list(registry_output.keys())\n",
    "\n",
    "    signatures_as_text = [\n",
    "        \" \".join(registry_output[h][\"signature\"].keys()) for h in hashes\n",
    "    ]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5))\n",
    "    matrix = vectorizer.fit_transform(signatures_as_text)\n",
    "\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=2,\n",
    "        metric=\"euclidean\",\n",
    "        copy=True,\n",
    "    )\n",
    "    labels = clusterer.fit_predict(matrix.toarray())\n",
    "\n",
    "    conjoined_map = {}\n",
    "    for i, cluster_id in enumerate(labels):\n",
    "        h = hashes[i]\n",
    "        conjoined_map[h] = {\n",
    "            \"cluster_id\": int(cluster_id),\n",
    "            \"keys\": list(registry_output[h][\"signature\"].keys()),\n",
    "            \"is_outlier\": cluster_id == -1,\n",
    "        }\n",
    "\n",
    "    sorted_dict = dict(sorted(conjoined_map.items()))\n",
    "    return sorted_dict\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "############################## Semantic Clustering ##############################\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "def conjoin_signatures_semantic(\n",
    "    registry_output: dict,\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "    cache_dir: str = \"./.models\",\n",
    "):\n",
    "    hashes = list(registry_output.keys())\n",
    "    registry_copy = deepcopy(registry_output)\n",
    "\n",
    "    signatures_as_text = []\n",
    "    for h in hashes:\n",
    "        h_dict = registry_copy[h][\"signature\"]\n",
    "        h_dict.pop(\"_unparsed\", None)\n",
    "        signatures_as_text.append(\", \".join(h_dict))\n",
    "\n",
    "    model = SentenceTransformer(embedding_model, cache_folder=cache_dir)\n",
    "    embeddings = model.encode(signatures_as_text)\n",
    "\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=2,\n",
    "        min_samples=1,\n",
    "        metric=\"cosine\",\n",
    "        cluster_selection_epsilon=0.18,\n",
    "        cluster_selection_method=\"eom\",\n",
    "        copy=True,\n",
    "    )\n",
    "    labels = clusterer.fit_predict(embeddings.astype(\"float64\"))\n",
    "\n",
    "    conjoined_map = {}\n",
    "    for i, cluster_id in enumerate(labels):\n",
    "        h = hashes[i]\n",
    "        conjoined_map[h] = {\n",
    "            \"cluster_id\": int(cluster_id),\n",
    "            \"keys\": list(registry_output[h][\"signature\"].keys()),\n",
    "            \"is_outlier\": cluster_id == -1,\n",
    "        }\n",
    "\n",
    "    return dict(sorted(conjoined_map.items()))\n",
    "\n",
    "\n",
    "# run structure clustering\n",
    "structure_cluster_map = conjoin_signatures(analyzer.signature_map)\n",
    "structure_clusters = {}\n",
    "for k, v in structure_cluster_map.items():\n",
    "    cluster_id = v[\"cluster_id\"]\n",
    "    keys = v[\"keys\"]\n",
    "    is_outlier = bool(v[\"is_outlier\"])\n",
    "    structure_clusters[cluster_id] = structure_clusters.get(cluster_id, [])\n",
    "    structure_clusters[cluster_id].append(\n",
    "        {\"signature_hash\": k, \"fields\": keys, \"is_outlier\": is_outlier}\n",
    "    )\n",
    "print(f\"Structural Clusters: \\n{json.dumps(structure_clusters, indent=4)}\\n\")\n",
    "\n",
    "\n",
    "# run semantic clustering\n",
    "print(json.dumps(analyzer.signature_map, indent=4))\n",
    "semantic_cluster_map = conjoin_signatures_semantic(analyzer.signature_map)\n",
    "semantic_clusters = {}\n",
    "for k, v in semantic_cluster_map.items():\n",
    "    cluster_id = v[\"cluster_id\"]\n",
    "    keys = v[\"keys\"]\n",
    "    is_outlier = bool(v[\"is_outlier\"])\n",
    "    semantic_clusters[cluster_id] = semantic_clusters.get(cluster_id, [])\n",
    "    semantic_clusters[cluster_id].append(\n",
    "        {\"signature_hash\": k, \"fields\": keys, \"is_outlier\": is_outlier}\n",
    "    )\n",
    "print(f\"Semantic Clusters: \\n{json.dumps(semantic_clusters, indent=4)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb800c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [{'signature_hash': '28d9f3b14d0e5516a186062212502d0c',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1014: Buyer=Rachel Kim, Locadtion=Seattle, WA, Total=$89.50, Items: coffee maker',\n",
       "   'parsed': {'order': '1014',\n",
       "    'buyer': 'Rachel Kim',\n",
       "    'locadtion': 'Seattle, WA',\n",
       "    'total': '$89.50',\n",
       "    'items': 'coffee maker'}},\n",
       "  {'signature_hash': '461a895ef9c5046dd2cb5026b6a62de0',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1017: Buyer=Griffin Arora, Location=Columbia, SC, Total=$512.00, Items: monitor, desk lamp, Discount: yes',\n",
       "   'parsed': {'order': '1017',\n",
       "    'buyer': 'Griffin Arora',\n",
       "    'location': 'Columbia, SC',\n",
       "    'total': '$512.00',\n",
       "    'items': 'monitor, desk lamp',\n",
       "    'discount': 'yes'}},\n",
       "  {'signature_hash': '50eb97a85647221ecc7f65f74d68d156',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1005: Buyer=Raj Patel, Total=1,200.50, Items: monitor, stand, cable',\n",
       "   'parsed': {'order': '1005',\n",
       "    'buyer': 'Raj Patel',\n",
       "    'total': '1,200.50',\n",
       "    'items': 'monitor, stand, cable'}},\n",
       "  {'signature_hash': '50eb97a85647221ecc7f65f74d68d156',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order=1016, Buyer=Jake Myers, Total=$1,512.00, Items: monitor,',\n",
       "   'parsed': {'order': '1016',\n",
       "    'buyer': 'Jake Myers',\n",
       "    'total': '$1,512.00',\n",
       "    'items': 'monitor'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1001: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable',\n",
       "   'parsed': {'order': '1001',\n",
       "    'buyer': 'John Davis',\n",
       "    'location': 'Columbus, OH',\n",
       "    'total': '$742.10',\n",
       "    'items': 'laptop, hdmi cable'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1004:   Buyer=  AMANDA SMITH ,Location=Seattle, WA,Total=$50.00, Items: desk lamp',\n",
       "   'parsed': {'order': '1004',\n",
       "    'buyer': 'AMANDA SMITH',\n",
       "    'location': 'Seattle, WA',\n",
       "    'total': '$50.00',\n",
       "    'items': 'desk lamp'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1006: total=$89.99, location=Miami, FL, buyer=Elena Rossi, Items: keyboard',\n",
       "   'parsed': {'order': '1006',\n",
       "    'total': '$89.99',\n",
       "    'location': 'Miami, FL',\n",
       "    'buyer': 'Elena Rossi',\n",
       "    'items': 'keyboard'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1007: Buyer=Chris P., Location=Denver, CO, Total=$12.00, Items: stickers -- [DISCOUNT APPLIED]',\n",
       "   'parsed': {'order': '1007',\n",
       "    'buyer': 'Chris P.',\n",
       "    'location': 'Denver, CO',\n",
       "    'total': '$12.00',\n",
       "    'items': 'stickers -- [DISCOUNT APPLIED]'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': \"Order 1008: Buyer=O'Connor, S., Location=Portland, OR, Total=$0.00, Items: \",\n",
       "   'parsed': {'order': '1008',\n",
       "    'buyer': \"O'Connor, S.\",\n",
       "    'location': 'Portland, OR',\n",
       "    'total': '$0.00',\n",
       "    'items': 'None'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1011: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable',\n",
       "   'parsed': {'order': '1011',\n",
       "    'buyer': 'John Davis',\n",
       "    'location': 'Columbus, OH',\n",
       "    'total': '$742.10',\n",
       "    'items': 'laptop, hdmi cable'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1012: Buyer=Sarah Liu, Location=Austin, TX, Total=$156.55, Items: headphones',\n",
       "   'parsed': {'order': '1012',\n",
       "    'buyer': 'Sarah Liu',\n",
       "    'location': 'Austin, TX',\n",
       "    'total': '$156.55',\n",
       "    'items': 'headphones'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1013: Buyer=Mike Turner, Location=Cleveland, OH, Total=$1299.99, Items: gaming pc, mouse',\n",
       "   'parsed': {'order': '1013',\n",
       "    'buyer': 'Mike Turner',\n",
       "    'location': 'Cleveland, OH',\n",
       "    'total': '$1299.99',\n",
       "    'items': 'gaming pc, mouse'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1015: Buyer=Chris Myers, Location=Cincinnati, OH, Total=$512.00, Items: monitor, desk lamp',\n",
       "   'parsed': {'order': '1015',\n",
       "    'buyer': 'Chris Myers',\n",
       "    'location': 'Cincinnati, OH',\n",
       "    'total': '$512.00',\n",
       "    'items': 'monitor, desk lamp'}}],\n",
       " 2: [{'signature_hash': '2bcfb738a2056c5ef5543917c9f3ac53',\n",
       "   'structure_cluster_id': 0,\n",
       "   'raw': '{\"id\": 4, \"name\": \"Chen Wei\", \"email\": \"chen.wei@example.com\", \"isActive\": true, \"createdAt\": null}',\n",
       "   'parsed': {'id': 4,\n",
       "    'name': 'Chen Wei',\n",
       "    'email': 'chen.wei@example.com',\n",
       "    'isActive': True,\n",
       "    'createdAt': None}},\n",
       "  {'signature_hash': '4286ed8928a42e13682233f00079d106',\n",
       "   'structure_cluster_id': 0,\n",
       "   'raw': '{\"id\": \"usr_002\", \"name\": \"Maria Lopez\", \"email\": \"maria.lopez@example.com\", \"role\": \"editor\", \"isActive\": null, \"createdAt\": \"2025-12-18T16:47:10Z\", \"lastLoginIp\": \"192.168.1.42\"}',\n",
       "   'parsed': {'id': 'usr_002',\n",
       "    'name': 'Maria Lopez',\n",
       "    'email': 'maria.lopez@example.com',\n",
       "    'role': 'editor',\n",
       "    'isActive': None,\n",
       "    'createdAt': '2025-12-18T16:47:10Z',\n",
       "    'lastLoginIp': '192.168.1.42'}},\n",
       "  {'signature_hash': 'a0713c61038426204fef9da85c91686f',\n",
       "   'structure_cluster_id': 0,\n",
       "   'raw': '{\"id\": \"usr_001\", \"name\": \"Alex Johnson\", \"role\": \"admin\", \"isActive\": true, \"createdAt\": \"2025-11-02T09:14:23Z\"}',\n",
       "   'parsed': {'id': 'usr_001',\n",
       "    'name': 'Alex Johnson',\n",
       "    'role': 'admin',\n",
       "    'isActive': True,\n",
       "    'createdAt': '2025-11-02T09:14:23Z'}},\n",
       "  {'signature_hash': 'd2d16f7c3698c6195ddaeb6205139150',\n",
       "   'structure_cluster_id': 0,\n",
       "   'raw': '{\"id\": \"usr_005\", \"name\": \"Broken Record\", \"email\": \"broken@example.com\"}',\n",
       "   'parsed': {'id': 'usr_005',\n",
       "    'name': 'Broken Record',\n",
       "    'email': 'broken@example.com'}},\n",
       "  {'signature_hash': 'd5d811706d95e2f6c1b713aa3fd1d9ab',\n",
       "   'structure_cluster_id': 0,\n",
       "   'raw': '{\"id\": \"usr_003\", \"email\": \"samir.patel@example.com\", \"role\": \"viewer\", \"isActive\": false, \"createdAt\": \"08/05/2024\"}',\n",
       "   'parsed': {'id': 'usr_003',\n",
       "    'email': 'samir.patel@example.com',\n",
       "    'role': 'viewer',\n",
       "    'isActive': False,\n",
       "    'createdAt': '08/05/2024'}}],\n",
       " 1: [{'signature_hash': '3baae1f59cac077e89e2f0b7d47a36cf',\n",
       "   'structure_cluster_id': 3,\n",
       "   'raw': '2026-01-30 14:22:01 INFO User login successful user_id=123',\n",
       "   'parsed': {'user_id': '123',\n",
       "    '_unparsed': '2026-01-30 14:22:01 INFO User login successful'}},\n",
       "  {'signature_hash': 'c2aebc126ff252eb9dae084d659d2cab',\n",
       "   'structure_cluster_id': 2,\n",
       "   'raw': 'level =INFO, user =Sam, id=1',\n",
       "   'parsed': {'level': 'INFO', 'user': 'Sam', 'id': '1'}}],\n",
       " -1: [{'signature_hash': '74db6b18a3e440d4dd054614492fa510',\n",
       "   'structure_cluster_id': 2,\n",
       "   'raw': 'timestamp=2026-01-30T14:22:01Z level=INFO user=alice action=login success=true',\n",
       "   'parsed': {'timestamp': '2026-01-30T14:22:01Z',\n",
       "    'level': 'INFO',\n",
       "    'user': 'alice',\n",
       "    'action': 'login',\n",
       "    'success': 'true'}},\n",
       "  {'signature_hash': 'd0f1c5f0782666cd8324b4cf57a3159d',\n",
       "   'structure_cluster_id': 2,\n",
       "   'raw': 'level=INFO cpu_usage=1,234.56 memory=512MB',\n",
       "   'parsed': {'level': 'INFO', 'cpu_usage': '1,234.56', 'memory': '512MB'}},\n",
       "  {'signature_hash': 'df87355cb94200d773396f5befa867d4',\n",
       "   'structure_cluster_id': 3,\n",
       "   'raw': '2026-01-30 14:22:01 INFO User login successful',\n",
       "   'parsed': {'_unparsed': '2026-01-30 14:22:01 INFO User login successful'}},\n",
       "  {'signature_hash': 'df87355cb94200d773396f5befa867d4',\n",
       "   'structure_cluster_id': 3,\n",
       "   'raw': '[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"',\n",
       "   'parsed': {'_unparsed': '[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"'}},\n",
       "  {'signature_hash': 'df87355cb94200d773396f5befa867d4',\n",
       "   'structure_cluster_id': 3,\n",
       "   'raw': '2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check',\n",
       "   'parsed': {'_unparsed': '2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check'}},\n",
       "  {'signature_hash': 'e8b13b0b419896ad0f402c15a762e2c1',\n",
       "   'structure_cluster_id': 3,\n",
       "   'raw': '{\"level\":\"INFO\",\"service\":\"orders\",\"order_id\":1001,\"status\":\"created\"}',\n",
       "   'parsed': {'level': 'INFO',\n",
       "    'service': 'orders',\n",
       "    'order_id': 1001,\n",
       "    'status': 'created'}}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# semantic cluster map to raw string, rough parsed dict, structure cluster, structure signature hash\n",
    "cluster_dict = {}\n",
    "for cluster_id, records in semantic_clusters.items():\n",
    "    cluster_dict[cluster_id] = cluster_dict.get(cluster_id, [])\n",
    "    for rec in records:\n",
    "        signature_hash = rec[\"signature_hash\"]\n",
    "        analyzer_records = analyzer.signature_map[signature_hash][\"records\"]\n",
    "        for r in analyzer_records:\n",
    "            cluster_dict[cluster_id].append(\n",
    "                {\n",
    "                    \"signature_hash\": signature_hash,\n",
    "                    \"structure_cluster_id\": structure_cluster_map[signature_hash].get(\n",
    "                        \"cluster_id\"\n",
    "                    ),\n",
    "                    \"raw\": r[\"raw\"],\n",
    "                    \"parsed\": r[\"parsed\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "cluster_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70354b4f",
   "metadata": {},
   "source": [
    "## LLM Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f369b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm connection\n",
    "API_KEY = os.getenv(\"LLM_API_KEY\")\n",
    "# api_base_url = os.getenv(\"LsLM_BASE_URL\")\n",
    "api_base_url = \"https://openrouter.ai/api/v1\"\n",
    "# model = \"openai/google/gemma-3-27b-it\"\n",
    "MODEL = \"openai/gpt-oss-120b:exacto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cea7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncLLMClient(\n",
    "    MODEL,\n",
    "    api_base_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc455a6a",
   "metadata": {},
   "source": [
    "## LLM Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "760f05a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "PYDANTIC_SYSTEM_PROMPT = \"\"\"You are a Data Architect. Your goal is to perform unsupervised schema inference on a sample of unstructured data.\n",
    "\n",
    "Generate a Pydantic `BaseModel` class that represents the \"Gold Standard\" foundation for this data pattern. \n",
    "\n",
    "Instructions:\n",
    "- Normalization: Suggest clean, snake_case keys for the identified fields.\n",
    "- If you see a string value for a field that follows a consistent structure (e.g., \"<city>, <state>\") then make sure that structure is accurately typed in the BaseModel.\n",
    "- Determine what fields should be required vs optional based on overall semantic meaning of the entity you are creating a BaseModel class for.\n",
    "\n",
    "Constraints:\n",
    "- Include a detailed description for each field using the `Field` class to explain what the field is and if there are any expected structural patterns (e.g., `state` should be two letters).\n",
    "- Create supplemental BaseModel classes where necessary to preserve semantic clarity.\n",
    "- Do NOT include any regex.\n",
    "- You MUST wrap your code in a python block with the following start marking \"```python\" and end marking \"```\".\n",
    "- If a field appears in some rows but not others, mark it as `Optional`.\n",
    "- You are only allowed to use the following imports: \"from typing import List, Dict, Optional; from pydantic import BaseModel, Field\".\n",
    "- Return ONLY the Pydantic class definitions (you are allowed to generate multiple as long as they are logically linked).\n",
    "\"\"\"\n",
    "\n",
    "PYDANTIC_USER_PROMPT = \"\"\"Please analyze the following representative samples of a new data pattern and generate the Pydantic 'Foundation' model.\n",
    "\n",
    "### Data Samples:\n",
    "{samples}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c08fc",
   "metadata": {},
   "source": [
    "## Generate Pydantic BaseModel Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "303d5e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"code_string\":\"```python\\nfrom typing import List, Optional\\nfrom pydantic import BaseModel, Field\\n\\nclass Location(BaseModel):\\n    city: str = Field(..., description=\\\"City name extracted from the location field.\\\")\\n    state: str = Field(..., description=\\\"Two-letter US state abbreviation, e.g., 'WA'.\\\")\\n\\nclass Order(BaseModel):\\n    order_id: int = Field(..., description=\\\"Numeric identifier of the order.\\\")\\n    buyer_name: str = Field(..., description=\\\"Full name of the buyer as it appears in the record.\\\")\\n    location: Location = Field(..., description=\\\"Geographic location of the buyer.\\\")\\n    total_amount: float = Field(..., description=\\\"Total monetary amount of the order in US dollars.\\\")\\n    items: List[str] = Field(..., description=\\\"List of item names included in the order. May be empty.\\\")\\n    discount_applied: Optional[bool] = Field(None, description=\\\"Indicates whether a discount was applied to the order.\\\")\\n```\",\"entrypoint_class_name\":\"Order\"}\n",
      "Successfully wrote code to order_base_model.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ModelResponseStructure(BaseModel):\n",
    "    code_string: str = Field(..., description=\"generated python code\")\n",
    "    entrypoint_class_name: str = Field(\n",
    "        ..., description=\"name of entrypoint base model class in the code generated\"\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_python_code(text):\n",
    "    \"\"\"\n",
    "    Extracts the Python code block from a string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted source code or an empty string if not found.\n",
    "    \"\"\"\n",
    "    block_pattern = r\"```(?:python)?\\s*(.*?)\\s*```\"\n",
    "    match = re.search(block_pattern, text, re.DOTALL)\n",
    "\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "\n",
    "for c_id, records in cluster_dict.items():\n",
    "    string_li = [r[\"raw\"] for r in records]\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": PYDANTIC_SYSTEM_PROMPT},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": PYDANTIC_USER_PROMPT.format(\n",
    "                samples=string_li,\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    buffer = []\n",
    "    response = await client.chat(\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        temperature=0.0,\n",
    "        response_format=ModelResponseStructure,\n",
    "    )\n",
    "    async for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            print(content, end=\"\", flush=True)\n",
    "            buffer.append(content)\n",
    "\n",
    "    resp = \"\".join(buffer)\n",
    "    resp: ModelResponseStructure = ModelResponseStructure(**json.loads(resp))\n",
    "\n",
    "    if not resp.code_string.startswith(\"```python\"):\n",
    "        resp.code_string = f\"```python\\n{resp.code_string}\\n```\"\n",
    "\n",
    "    code = extract_python_code(resp.code_string)\n",
    "\n",
    "    namespace = {}\n",
    "    exec(code, globals(), namespace)\n",
    "\n",
    "    # access the function from the namespace dictionary\n",
    "    cls = namespace.get(resp.entrypoint_class_name)\n",
    "    cls.model_rebuild(_types_namespace=namespace)\n",
    "    schema = cls.model_json_schema()\n",
    "\n",
    "    fname = f\"{resp.entrypoint_class_name.lower()}_base_model.py\"\n",
    "    code = code.encode(\"ascii\", errors=\"ignore\").decode(\"ascii\")\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(code)\n",
    "        print(f\"\\nSuccessfully wrote code to {fname}\")\n",
    "\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a80c7b",
   "metadata": {},
   "source": [
    "## Langgraph Robustness and Stategraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab909348",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECT_PROMPT = \"\"\"You are a Lead Data Architect.\n",
    "Define a simple Pydantic v2 `BaseModel` that represents the \"Gold Standard\" foundation for the data pattern found in the input samples.\n",
    "\n",
    "INPUT SAMPLES (Multiple variations):\n",
    "{samples}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Normalization: Suggest clean, snake_case keys for the identified fields.\n",
    "2. Optionality: If a field is missing in ANY of the samples, you MUST wrap it in Optional[...].\n",
    "3. Determine what fields should be required vs optional based on overall semantic meaning of the entity you are creating a BaseModel class for.\n",
    "4. Include a detailed description for each field using the `Field` class to explain what the field is and if there are any expected structural patterns (e.g., `state` should be two letters).\n",
    "5. Do NOT include any regex.\n",
    "6. You MUST wrap your code in a python block with the following start marking \"```python\" and end marking \"```\".\n",
    "7. Create supplemental BaseModel classes where necessary to preserve semantic clarity.\n",
    "8. You are ONLY allowed to use the following imports: \"from typing import List, Dict, Optional; from pydantic import BaseModel, Field\".\n",
    "9. Keep primary keys as type string.\n",
    "10. Infer best data type from string value (e.g., money should be a float, \"true/false\" or \"yes/no\" fields should be a boolean, and fields that represent multiple entities should use a representative aggregate data structure type)\n",
    "11. NEVER set potentially boolean fields as optional. Instead, when not explicitly declared, infer as to what the default value ought based on the semantic meaning of the field and how it appears in the samples that do provide it.\n",
    "12. Perform semantic merging: Identify fields across structural variants that share the same intent and conjoin them under a single, definitive schema key to avoid redundancy (e.g., \"location\" vs \"city\", \"state\", \"zip code\")\n",
    "\n",
    "Return ONLY the Python code for the class. Include necessary imports (from pydantic import BaseModel, Field, etc.).\n",
    "\"\"\"\n",
    "\n",
    "CODER_PROMPT = \"\"\"You are a Senior Data Engineer.\n",
    "Your task is to write a concise but effective transformation function `transform_to_models(parsed_dict: list[dict]) -> list[dict]` that maps roughly parsed dictionaries into the provided pydantic v2 target schema base model definition.\n",
    "\n",
    "TARGET SCHEMA (Python Pydantic v2 BaseModel):\n",
    "{schema}\n",
    "\n",
    "SOURCE SAMPLES:\n",
    "{samples}\n",
    "\n",
    "Logic Requirements:\n",
    "1. Use a 'coalesce' approach: for each target field, check all possible source keys from the input dictionary samples.\n",
    "2. Use parsed_dict.get() for optional fields.\n",
    "3. Infer best data type from string (e.g., \"$120.00\" should be a float, and \"true\" should be a boolean). \n",
    "4. ALL python code must be encapsulated by the `transform_to_models()` function -- if it's not in that function it will not be run.\n",
    "\n",
    "Return ONLY the Python code for the function `transform_to_models`. Do not include the Pydantic class in your response.\n",
    "\"\"\"\n",
    "\n",
    "CODE_EXECUTION = \"\"\"\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import *\n",
    "import json, re\n",
    "\n",
    "{schema}\n",
    "\n",
    "{parser_code}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a80d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaero/projects/interviews/team_raft/dq-swirl/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import operator\n",
    "from typing import List, TypedDict, Optional, Literal, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from dq_swirl.utils.log_utils import get_custom_logger \n",
    "from typing import Annotated\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "import traceback\n",
    "\n",
    "\n",
    "logger = get_custom_logger()\n",
    "\n",
    "class ModelResponseStructure(BaseModel):\n",
    "    code_string: str = Field(..., description=\"generated python code\")\n",
    "    entrypoint_class_name: str = Field(\n",
    "        ..., \n",
    "        description=\"name of entrypoint base model class in the code generated\",\n",
    "    )\n",
    "\n",
    "class MultiAgentState(TypedDict):\n",
    "    semantic_id: str\n",
    "    structure_cluster_id: str\n",
    "    data_pairs_all: List[Dict[str, Any]]\n",
    "    data_pairs_structure: List[Dict[str, Any]]\n",
    "    # Reducers: 'new' replaces 'old' for strings/objects, but we sum 'attempts'\n",
    "    gold_schema: Annotated[Optional[ModelResponseStructure], lambda old, new: new]\n",
    "    parser_code: Annotated[Optional[str], lambda old, new: new]\n",
    "    feedback: Annotated[Optional[str], lambda old, new: new]\n",
    "    error_type: Annotated[Optional[Literal[\"SCHEMA_ISSUE\", \"CODE_ISSUE\"]], lambda old, new: new]\n",
    "    attempts: Annotated[int, operator.add] # Use addition to\n",
    "\n",
    "async def architect_node(state: MultiAgentState):\n",
    "    if state.get(\"gold_schema\") and state.get(\"error_type\") != \"SCHEMA_ISSUE\": \n",
    "        return {\"attempts\": 0}\n",
    "\n",
    "    logger.info(f\"[Architect] Defining Semantic Goal: {state['attempts']}\")\n",
    "    # Send a variety of samples so the architect sees all potential fields\n",
    "    samples = json.dumps([p['parsed'] for p in state['data_pairs_all'][:100]], indent=2)\n",
    "    \n",
    "    prompt = ARCHITECT_PROMPT.format(\n",
    "        samples=samples,\n",
    "    )\n",
    "    buffer = []\n",
    "    response = await client.chat(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        api_key_override=API_KEY,\n",
    "        stream=True,\n",
    "        temperature=0.0,\n",
    "        response_format=ModelResponseStructure\n",
    "    )\n",
    "    async for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            buffer.append(content)\n",
    "\n",
    "    resp = \"\".join(buffer)\n",
    "    resp = ModelResponseStructure(**json.loads(resp))\n",
    "    resp.code_string = extract_python_code(resp.code_string)\n",
    "    print(resp.code_string )\n",
    "\n",
    "    return {\n",
    "        \"gold_schema\": resp, \n",
    "        \"attempts\": 1,\n",
    "        \"feedback\": None,\n",
    "        \"error_type\": None    # CLEAR the error type\n",
    "    }\n",
    "\n",
    "async def schema_tester_node(state: MultiAgentState):\n",
    "    logger.info(f\"[Scehma Tester] Validating Functional BaseModel: {state['attempts']}\")\n",
    "    python_base_model_str = state[\"gold_schema\"].code_string\n",
    "    \n",
    "    env = {}\n",
    "    try:\n",
    "        exec(python_base_model_str, globals(), env)\n",
    "        cls_name = state[\"gold_schema\"].entrypoint_class_name\n",
    "        model = env[cls_name]\n",
    "        \n",
    "        model.model_rebuild(_types_namespace=env)\n",
    "        schema = model.model_json_schema()\n",
    "            \n",
    "        return {\"feedback\": \"SUCCESS\"}\n",
    "    except Exception as e:\n",
    "        err_msg = traceback.format_exc()\n",
    "        logger.exception(e)\n",
    "        return {\"feedback\": err_msg, \"error_type\": \"SCHEMA_ISSUE\"}\n",
    "\n",
    "async def coder_node(state: MultiAgentState):\n",
    "    logger.info(f\"[Coder] Parser for Gold Schema: {state['attempts']}\")\n",
    "    samples = json.dumps([rec[\"parsed\"] for rec in state[\"data_pairs_structure\"]], indent=2)\n",
    "    \n",
    "    prompt = CODER_PROMPT.format(\n",
    "        schema=state[\"gold_schema\"].code_string,\n",
    "        samples=samples\n",
    "    )\n",
    "    buffer = []\n",
    "    response = await client.chat(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        api_key_override=API_KEY,\n",
    "        stream=True,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    async for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            print(content, end=\"\", flush=True)\n",
    "            buffer.append(content)\n",
    "\n",
    "    resp = \"\".join(buffer)\n",
    "    code = extract_python_code(resp)\n",
    "\n",
    "    return {\n",
    "        \"parser_code\": code, \n",
    "        \"attempts\": 1,\n",
    "        \"feedback\": None, \n",
    "        \"error_type\": None\n",
    "    }\n",
    "\n",
    "async def code_tester_node(state: MultiAgentState):\n",
    "    logger.info(f\"[Code Tester] Stress-testing parser: {state['attempts']}\")\n",
    "    full_code = CODE_EXECUTION.format(\n",
    "        schema=state[\"gold_schema\"].code_string,\n",
    "        parser_code=state[\"parser_code\"]\n",
    "    )\n",
    "    \n",
    "    env = {}\n",
    "    try:\n",
    "        cls_name = state[\"gold_schema\"].entrypoint_class_name        \n",
    "        exec(full_code, globals(), env)\n",
    "        func = env[\"transform_to_models\"]\n",
    "        model = env[cls_name]\n",
    "        model.model_rebuild(_types_namespace=env)\n",
    "\n",
    "        input_data = [pair['parsed'] for pair in state['data_pairs_structure']]\n",
    "        mapped_batch = func(input_data)\n",
    "        for mapped_dict in mapped_batch:\n",
    "            model.model_validate(mapped_dict)\n",
    "            print(f\"Input: {mapped_dict} -- PASSED\")\n",
    "        return {\"feedback\": \"SUCCESS\"}\n",
    "    except Exception as e:\n",
    "        err_msg = traceback.format_exc()\n",
    "        try:\n",
    "            print(f\"Input: {mapped_dict} -- FAILED\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        return {\"feedback\": err_msg, \"error_type\": \"CODE_ISSUE\"}\n",
    "\n",
    "async def exporter_node(state: MultiAgentState):\n",
    "    base_model_name = state[\"gold_schema\"].entrypoint_class_name\n",
    "    filename = f\"parser_{base_model_name}_struct_{state['structure_cluster_id']}.py\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(f\"{state['gold_schema'].code_string}\\n\\n{state['parser_code']}\")\n",
    "    print(f\"--> Exported: {filename}\")\n",
    "    return {\"feedback\": \"DONE\"}\n",
    "\n",
    "def schema_router(state: MultiAgentState):\n",
    "    \"\"\"Determines if we move to Coder or retry the Architect.\n",
    "    \"\"\"\n",
    "    feedback = state.get(\"feedback\")\n",
    "    attempts = state.get(\"attempts\", 0)\n",
    "\n",
    "    if feedback == \"SUCCESS\":\n",
    "        return \"coder\"\n",
    "    \n",
    "    # if failed too many times, just stop the process\n",
    "    if attempts >= 3:\n",
    "        logger.error(f\"Schema failed after {attempts} attempts. Aborting.\")\n",
    "        return \"end\"\n",
    "    \n",
    "    return \"architect\"\n",
    "\n",
    "def code_router(state: MultiAgentState):\n",
    "    \"\"\"Determines if we export or retry Coder/Architect.\n",
    "    \"\"\"\n",
    "    feedback = state.get(\"feedback\")\n",
    "    error_type = state.get(\"error_type\")\n",
    "    attempts = state.get(\"attempts\", 0)\n",
    "\n",
    "    if feedback == \"SUCCESS\":\n",
    "        return \"exporter\"\n",
    "    \n",
    "    if attempts >= 6:\n",
    "        return \"end\"\n",
    "    \n",
    "    # Specific routing based on where the failure happened\n",
    "    if error_type == \"SCHEMA_ISSUE\":\n",
    "        return \"architect\" \n",
    "    \n",
    "    # Default to retrying the coder for CODE_ISSUE or unknown errors\n",
    "    return \"coder\"\n",
    "\n",
    "\n",
    "\n",
    "## Define Graph\n",
    "workflow = StateGraph(MultiAgentState)\n",
    "workflow.add_node(\"architect\", architect_node) \n",
    "workflow.add_node(\"schema_tester\", schema_tester_node) \n",
    "workflow.add_node(\"coder\", coder_node)\n",
    "workflow.add_node(\"code_tester\", code_tester_node)\n",
    "workflow.add_node(\"exporter\", exporter_node)\n",
    "\n",
    "workflow.add_edge(START, \"architect\")\n",
    "workflow.add_edge(\"architect\", \"schema_tester\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"schema_tester\", \n",
    "    schema_router, \n",
    "    {\n",
    "        \"architect\": \"architect\", \n",
    "        \"coder\": \"coder\", \n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"coder\", \"code_tester\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"code_tester\", \n",
    "    code_router, \n",
    "    {\n",
    "        \"architect\": \"architect\", \n",
    "        \"coder\": \"coder\", \n",
    "        \"exporter\": \"exporter\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"exporter\", END)\n",
    "\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d34e14de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:14:46\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:42\u001b[0m [Architect] Defining Semantic Goal: 0\n",
      "\u001b[38;5;245m2026-02-02 00:14:50\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:80\u001b[0m [Scehma Tester] Validating Functional BaseModel: 1\n",
      "\u001b[38;5;245m2026-02-02 00:14:50\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:99\u001b[0m [Coder] Parser for Gold Schema: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List, Optional\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "class Order(BaseModel):\n",
      "    \"\"\"Gold Standard representation of an e‑commerce order.\"\"\"\n",
      "    order: str = Field(..., description=\"Unique identifier for the order, kept as a string.\")\n",
      "    buyer: str = Field(..., description=\"Name of the customer who placed the order.\")\n",
      "    location: Optional[str] = Field(None, description=\"City and state of the shipping address in the format 'City, ST'.\")\n",
      "    total: float = Field(..., description=\"Monetary total of the order expressed in US dollars.\")\n",
      "    items: List[str] = Field(..., description=\"List of product names purchased in the order. Items are split on commas.\")\n",
      "    discount: bool = Field(False, description=\"Flag indicating whether a discount was applied to the order.\")\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Convert a list of loosely‑structured order dictionaries into a list of dictionaries\n",
      "    that conform to the target Pydantic schema.\n",
      "\n",
      "    The function follows a “coalesce” strategy for each field, handling misspelled keys,\n",
      "    cleaning values, and inferring proper Python types.\n",
      "    \"\"\"\n",
      "    import re\n",
      "\n",
      "    def _to_float(val: str) -> float:\n",
      "        \"\"\"Strip currency symbols/commas and convert to float.\"\"\"\n",
      "        if not isinstance(val, str):\n",
      "            return float(val)\n",
      "        cleaned = val.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
      "        try:\n",
      "            return float(cleaned)\n",
      "        except ValueError:\n",
      "            return 0.0\n",
      "\n",
      "    def _to_bool(val) -> bool:\n",
      "        \"\"\"Interpret common truthy strings as True.\"\"\"\n",
      "        if isinstance(val, bool):\n",
      "            return val\n",
      "        if isinstance(val, (int, float)):\n",
      "            return bool(val)\n",
      "        if not isinstance(val, str):\n",
      "            return False\n",
      "        return val.strip().lower() in {\"yes\", \"true\", \"y\", \"1\"}\n",
      "\n",
      "    def _parse_items(raw: str) -> list[str]:\n",
      "        \"\"\"Extract a clean list of item names from the raw string.\"\"\"\n",
      "        if not isinstance(raw, str):\n",
      "            return []\n",
      "        # Remove any trailing notes like \"-- [DISCOUNT APPLIED]\"\n",
      "        raw = raw.split(\"--\")[0]\n",
      "        # Split on commas\n",
      "        parts = [p.strip() for p in raw.split(\",\")]\n",
      "        # Filter out empty strings and the literal \"None\"\n",
      "        return [p for p in parts if p and p.lower() != \"none\"]\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "\n",
      "    for src in parsed_dict:\n",
      "        # ----- order (required) -----\n",
      "        order = src.get(\"order\")\n",
      "        # ----- buyer (required) -----\n",
      "        buyer = src.get(\"buyer\")\n",
      "        # ----- location (optional, coalesce misspelling) -----\n",
      "        location = src.get(\"location\") or src.get(\"locadtion\")\n",
      "        # ----- total (required, infer float) -----\n",
      "        total_raw = src.get(\"total\", 0)\n",
      "        total = _to_float(total_raw)\n",
      "        # ----- items (required, split on commas) -----\n",
      "        items_raw = src.get(\"items\", \"\")\n",
      "        items = _parse_items(items_raw)\n",
      "\n",
      "        # ----- discount (optional, default False) -----\n",
      "        discount = False\n",
      "        # explicit discount field\n",
      "        if \"discount\" in src:\n",
      "            discount = _to_bool(src[\"discount\"])\n",
      "        # infer from items annotation\n",
      "        elif isinstance(items_raw, str) and \"[discount applied]\" in items_raw.lower():\n",
      "            discount = True\n",
      "\n",
      "        transformed.append(\n",
      "            {\n",
      "                \"order\": order,\n",
      "                \"buyer\": buyer,\n",
      "                \"location\": location,\n",
      "                \"total\": total,\n",
      "                \"items\": items,\n",
      "                \"discount\": discount,\n",
      "            }\n",
      "        )\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:14:52\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:135\u001b[0m [Code Tester] Stress-testing parser: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'order': '1014', 'buyer': 'Rachel Kim', 'location': 'Seattle, WA', 'total': 89.5, 'items': ['coffee maker'], 'discount': False} -- PASSED\n",
      "Input: {'order': '1017', 'buyer': 'Griffin Arora', 'location': 'Columbia, SC', 'total': 512.0, 'items': ['monitor', 'desk lamp'], 'discount': True} -- PASSED\n",
      "Input: {'order': '1005', 'buyer': 'Raj Patel', 'location': None, 'total': 1200.5, 'items': ['monitor', 'stand', 'cable'], 'discount': False} -- PASSED\n",
      "Input: {'order': '1016', 'buyer': 'Jake Myers', 'location': None, 'total': 1512.0, 'items': ['monitor'], 'discount': False} -- PASSED\n",
      "Input: {'order': '1001', 'buyer': 'John Davis', 'location': 'Columbus, OH', 'total': 742.1, 'items': ['laptop', 'hdmi cable'], 'discount': False} -- PASSED\n",
      "Input: {'order': '1004', 'buyer': 'AMANDA SMITH', 'location': 'Seattle, WA', 'total': 50.0, 'items': ['desk lamp'], 'discount': False} -- PASSED\n",
      "Input: {'order': '1006', 'buyer': 'Elena Rossi', 'location': 'Miami, FL', 'total': 89.99, 'items': ['keyboard'], 'discount': False} -- PASSED\n",
      "Input: {'order': '1007', 'buyer': 'Chris P.', 'location': 'Denver, CO', 'total': 12.0, 'items': ['stickers'], 'discount': True} -- PASSED\n",
      "Input: {'order': '1008', 'buyer': \"O'Connor, S.\", 'location': 'Portland, OR', 'total': 0.0, 'items': [], 'discount': False} -- PASSED\n",
      "Input: {'order': '1011', 'buyer': 'John Davis', 'location': 'Columbus, OH', 'total': 742.1, 'items': ['laptop', 'hdmi cable'], 'discount': False} -- PASSED\n",
      "Input: {'order': '1012', 'buyer': 'Sarah Liu', 'location': 'Austin, TX', 'total': 156.55, 'items': ['headphones'], 'discount': False} -- PASSED\n",
      "Input: {'order': '1013', 'buyer': 'Mike Turner', 'location': 'Cleveland, OH', 'total': 1299.99, 'items': ['gaming pc', 'mouse'], 'discount': False} -- PASSED\n",
      "Input: {'order': '1015', 'buyer': 'Chris Myers', 'location': 'Cincinnati, OH', 'total': 512.0, 'items': ['monitor', 'desk lamp'], 'discount': False} -- PASSED\n",
      "--> Exported: parser_Order_struct_1.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:14:52\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3536762348.py:26\u001b[0m --- Finished Cluster 1 ---\n",
      "\u001b[38;5;245m2026-02-02 00:14:52\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:42\u001b[0m [Architect] Defining Semantic Goal: 0\n",
      "\u001b[38;5;245m2026-02-02 00:14:56\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:80\u001b[0m [Scehma Tester] Validating Functional BaseModel: 1\n",
      "\u001b[38;5;245m2026-02-02 00:14:56\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:99\u001b[0m [Coder] Parser for Gold Schema: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import Optional\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "class UserBase(BaseModel):\n",
      "    \"\"\"Gold standard user entity.\"\"\"\n",
      "    id: str = Field(..., description=\"Unique identifier for the user, stored as a string.\")\n",
      "    name: Optional[str] = Field(None, description=\"Full name of the user.\")\n",
      "    email: Optional[str] = Field(None, description=\"User's email address.\")\n",
      "    is_active: bool = Field(False, description=\"Indicates whether the user account is active.\")\n",
      "    created_at: Optional[str] = Field(None, description=\"Timestamp when the user was created, ISO 8601 format if available.\")\n",
      "    role: Optional[str] = Field(None, description=\"Role assigned to the user, e.g., admin, editor, viewer.\")\n",
      "    last_login_ip: Optional[str] = Field(None, description=\"IP address from which the user last logged in.\")\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Convert loosely‑parsed user dicts into the strict shape required by the\n",
      "    ``UserBase`` pydantic model.\n",
      "\n",
      "    The function:\n",
      "    * coalesces multiple possible source keys for each target field,\n",
      "    * casts values to the appropriate Python types,\n",
      "    * falls back to the model defaults when a value is missing or null.\n",
      "    \"\"\"\n",
      "    from typing import Any\n",
      "\n",
      "    # Mapping of target field -> possible source keys (ordered by priority)\n",
      "    FIELD_MAP = {\n",
      "        \"id\": [\"id\"],\n",
      "        \"name\": [\"name\", \"fullName\"],\n",
      "        \"email\": [\"email\", \"emailAddress\"],\n",
      "        \"is_active\": [\"isActive\", \"is_active\", \"active\"],\n",
      "        \"created_at\": [\"createdAt\", \"created_at\", \"created\"],\n",
      "        \"role\": [\"role\", \"userRole\"],\n",
      "        \"last_login_ip\": [\"lastLoginIp\", \"last_login_ip\", \"lastLoginIP\"],\n",
      "    }\n",
      "\n",
      "    def _to_bool(v: Any) -> bool:\n",
      "        if isinstance(v, bool):\n",
      "            return v\n",
      "        if isinstance(v, (int, float)):\n",
      "            return bool(v)\n",
      "        if isinstance(v, str):\n",
      "            low = v.strip().lower()\n",
      "            if low in {\"true\", \"1\", \"yes\", \"y\"}:\n",
      "                return True\n",
      "            if low in {\"false\", \"0\", \"no\", \"n\"}:\n",
      "                return False\n",
      "        return False  # default when conversion fails\n",
      "\n",
      "    def _coerce(value: Any, target: str) -> Any:\n",
      "        \"\"\"Cast *value* to the type expected by *target*.\"\"\"\n",
      "        if value is None:\n",
      "            return None\n",
      "        if target == \"id\":\n",
      "            return str(value)\n",
      "        if target == \"is_active\":\n",
      "            return _to_bool(value)\n",
      "        # For all other fields we keep the raw string representation\n",
      "        return str(value)\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "    for src in parsed_dict:\n",
      "        out: dict = {}\n",
      "        for tgt, keys in FIELD_MAP.items():\n",
      "            # find first non‑null value among possible source keys\n",
      "            raw = next((src.get(k) for k in keys if src.get(k) is not None), None)\n",
      "\n",
      "            # Apply defaults matching the pydantic schema\n",
      "            if raw is None:\n",
      "                if tgt == \"is_active\":\n",
      "                    out[tgt] = False\n",
      "                else:\n",
      "                    out[tgt] = None\n",
      "            else:\n",
      "                out[tgt] = _coerce(raw, tgt)\n",
      "\n",
      "        transformed.append(out)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:14:58\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:135\u001b[0m [Code Tester] Stress-testing parser: 2\n",
      "\u001b[38;5;245m2026-02-02 00:14:58\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3536762348.py:26\u001b[0m --- Finished Cluster 0 ---\n",
      "\u001b[38;5;245m2026-02-02 00:14:58\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:42\u001b[0m [Architect] Defining Semantic Goal: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'id': '4', 'name': 'Chen Wei', 'email': 'chen.wei@example.com', 'is_active': True, 'created_at': None, 'role': None, 'last_login_ip': None} -- PASSED\n",
      "Input: {'id': 'usr_002', 'name': 'Maria Lopez', 'email': 'maria.lopez@example.com', 'is_active': False, 'created_at': '2025-12-18T16:47:10Z', 'role': 'editor', 'last_login_ip': '192.168.1.42'} -- PASSED\n",
      "Input: {'id': 'usr_001', 'name': 'Alex Johnson', 'email': None, 'is_active': True, 'created_at': '2025-11-02T09:14:23Z', 'role': 'admin', 'last_login_ip': None} -- PASSED\n",
      "Input: {'id': 'usr_005', 'name': 'Broken Record', 'email': 'broken@example.com', 'is_active': False, 'created_at': None, 'role': None, 'last_login_ip': None} -- PASSED\n",
      "Input: {'id': 'usr_003', 'name': None, 'email': 'samir.patel@example.com', 'is_active': False, 'created_at': '08/05/2024', 'role': 'viewer', 'last_login_ip': None} -- PASSED\n",
      "--> Exported: parser_UserBase_struct_0.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:01\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:80\u001b[0m [Scehma Tester] Validating Functional BaseModel: 1\n",
      "\u001b[38;5;245m2026-02-02 00:15:01\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:99\u001b[0m [Coder] Parser for Gold Schema: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import Optional\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "class LogEntry(BaseModel):\n",
      "    \"\"\"Gold Standard model representing a normalized log or user event.\"\"\"\n",
      "\n",
      "    user_id: str = Field(\n",
      "        ..., \n",
      "        description=\"Unique identifier for the user. Merged from 'user_id' and 'id' fields in source data.\"\n",
      "    )\n",
      "    user_name: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"Human‑readable name of the user when available.\"\n",
      "    )\n",
      "    level: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"Log level or severity indicator (e.g., INFO, WARN, ERROR).\"\n",
      "    )\n",
      "    unparsed: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"Original unparsed log line when provided.\"\n",
      "    )\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries\n",
      "    that match the ``LogEntry`` schema.\n",
      "\n",
      "    The function:\n",
      "    * Coalesces possible source keys for each target field.\n",
      "    * Uses ``dict.get`` for optional fields.\n",
      "    * Attempts to infer a more appropriate Python type from string values\n",
      "      (int → float → bool → original string).\n",
      "    \"\"\"\n",
      "    from typing import Any\n",
      "\n",
      "    def _coerce(value: Any) -> Any:\n",
      "        \"\"\"Best‑effort conversion of a string to int, float or bool.\"\"\"\n",
      "        if isinstance(value, str):\n",
      "            v = value.strip()\n",
      "            # Boolean\n",
      "            low = v.lower()\n",
      "            if low in {\"true\", \"false\"}:\n",
      "                return low == \"true\"\n",
      "            # Integer\n",
      "            try:\n",
      "                return int(v)\n",
      "            except ValueError:\n",
      "                pass\n",
      "            # Float\n",
      "            try:\n",
      "                return float(v)\n",
      "            except ValueError:\n",
      "                pass\n",
      "            return v  # keep as string\n",
      "        return value\n",
      "\n",
      "    # possible source keys for each target field\n",
      "    USER_ID_KEYS = (\"user_id\", \"id\")\n",
      "    USER_NAME_KEYS = (\"user_name\", \"name\", \"username\", \"user\")\n",
      "    LEVEL_KEYS = (\"level\", \"log_level\", \"severity\")\n",
      "    UNPARSED_KEYS = (\"_unparsed\", \"unparsed\", \"raw\", \"original\")\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "\n",
      "    for entry in parsed_dict:\n",
      "        # ----- required field: user_id -----\n",
      "        user_id = None\n",
      "        for k in USER_ID_KEYS:\n",
      "            if (val := entry.get(k)) is not None:\n",
      "                user_id = _coerce(val)\n",
      "                break\n",
      "        if user_id is None:\n",
      "            # Skip entries that lack a usable identifier – they cannot satisfy the schema.\n",
      "            continue\n",
      "\n",
      "        # ----- optional fields -----\n",
      "        def _first_available(keys: tuple[str, ...]) -> Any:\n",
      "            for k in keys:\n",
      "                if (val := entry.get(k)) is not None:\n",
      "                    return _coerce(val)\n",
      "            return None\n",
      "\n",
      "        user_name = _first_available(USER_NAME_KEYS)\n",
      "        level = _first_available(LEVEL_KEYS)\n",
      "        unparsed = _first_available(UNPARSED_KEYS)\n",
      "\n",
      "        transformed.append(\n",
      "            {\n",
      "                \"user_id\": user_id,\n",
      "                \"user_name\": user_name,\n",
      "                \"level\": level,\n",
      "                \"unparsed\": unparsed,\n",
      "            }\n",
      "        )\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:03\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:135\u001b[0m [Code Tester] Stress-testing parser: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': 123, 'user_name': None, 'level': None, 'unparsed': '2026-01-30 14:22:01 INFO User login successful'} -- FAILED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:03\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:99\u001b[0m [Coder] Parser for Gold Schema: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries that\n",
      "    conform to the ``LogEntry`` schema.\n",
      "\n",
      "    The function:\n",
      "    * Coalesces possible source keys for each target field.\n",
      "    * Infers a more appropriate Python type from string values (int, float, bool).\n",
      "    * Returns plain ``dict`` objects – suitable for feeding into the Pydantic model.\n",
      "    \"\"\"\n",
      "\n",
      "    def _coalesce(source: dict, candidates: list[str]) -> object | None:\n",
      "        \"\"\"Return the first non‑null value found for the given candidate keys.\"\"\"\n",
      "        for key in candidates:\n",
      "            if key in source and source[key] is not None:\n",
      "                return source[key]\n",
      "        return None\n",
      "\n",
      "    def _infer_type(value: object) -> object:\n",
      "        \"\"\"Best‑effort conversion of a string to int, float, or bool.\"\"\"\n",
      "        if not isinstance(value, str):\n",
      "            return value\n",
      "        v = value.strip()\n",
      "        # Boolean detection\n",
      "        low = v.lower()\n",
      "        if low == \"true\":\n",
      "            return True\n",
      "        if low == \"false\":\n",
      "            return False\n",
      "        # Integer detection\n",
      "        try:\n",
      "            i = int(v)\n",
      "            return i\n",
      "        except ValueError:\n",
      "            pass\n",
      "        # Float detection\n",
      "        try:\n",
      "            f = float(v)\n",
      "            return f\n",
      "        except ValueError:\n",
      "            pass\n",
      "        # Return original string if no conversion succeeded\n",
      "        return v\n",
      "\n",
      "    # Mapping of target fields to possible source keys\n",
      "    field_map = {\n",
      "        \"user_id\": [\"user_id\", \"id\"],\n",
      "        \"user_name\": [\"user_name\", \"name\", \"username\", \"user\"],\n",
      "        \"level\": [\"level\", \"log_level\", \"severity\"],\n",
      "        \"unparsed\": [\"_unparsed\", \"raw\", \"original\", \"unparsed\"],\n",
      "    }\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "    for src in parsed_dict:\n",
      "        entry: dict = {}\n",
      "        # user_id is required – if missing we still include None (Pydantic will raise later)\n",
      "        entry[\"user_id\"] = _infer_type(_coalesce(src, field_map[\"user_id\"]))\n",
      "        entry[\"user_name\"] = _infer_type(_coalesce(src, field_map[\"user_name\"]))\n",
      "        entry[\"level\"] = _infer_type(_coalesce(src, field_map[\"level\"]))\n",
      "        entry[\"unparsed\"] = _infer_type(_coalesce(src, field_map[\"unparsed\"]))\n",
      "        transformed.append(entry)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:05\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:135\u001b[0m [Code Tester] Stress-testing parser: 3\n",
      "\u001b[38;5;245m2026-02-02 00:15:05\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:99\u001b[0m [Coder] Parser for Gold Schema: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': 123, 'user_name': None, 'level': None, 'unparsed': '2026-01-30 14:22:01 INFO User login successful'} -- FAILED\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries that\n",
      "    conform to the ``LogEntry`` schema.\n",
      "\n",
      "    The function follows a *coalesce* strategy:\n",
      "        • ``user_id``  ← first non‑null value from [\"user_id\", \"id\"]\n",
      "        • ``user_name``← first non‑null value from [\"user_name\", \"name\", \"username\"]\n",
      "        • ``level``    ← first non‑null value from [\"level\", \"log_level\", \"severity\"]\n",
      "        • ``unparsed`` ← first non‑null value from [\"_unparsed\", \"raw\", \"original\"]\n",
      "\n",
      "    It also attempts to infer the most appropriate Python type for string values\n",
      "    (int, float, bool) while preserving strings that cannot be converted.\n",
      "    \"\"\"\n",
      "\n",
      "    def _infer_type(val):\n",
      "        \"\"\"Best‑effort conversion of a string to int, float, bool or leave as‑is.\"\"\"\n",
      "        if not isinstance(val, str):\n",
      "            return val\n",
      "        s = val.strip()\n",
      "        # Boolean detection\n",
      "        low = s.lower()\n",
      "        if low == \"true\":\n",
      "            return True\n",
      "        if low == \"false\":\n",
      "            return False\n",
      "        # Integer detection\n",
      "        try:\n",
      "            i = int(s)\n",
      "            return i\n",
      "        except ValueError:\n",
      "            pass\n",
      "        # Float detection\n",
      "        try:\n",
      "            f = float(s)\n",
      "            return f\n",
      "        except ValueError:\n",
      "            pass\n",
      "        # Return original string if no conversion succeeded\n",
      "        return s\n",
      "\n",
      "    # Mapping of target fields to possible source keys (ordered by priority)\n",
      "    field_map = {\n",
      "        \"user_id\":   [\"user_id\", \"id\"],\n",
      "        \"user_name\": [\"user_name\", \"name\", \"username\"],\n",
      "        \"level\":     [\"level\", \"log_level\", \"severity\"],\n",
      "        \"unparsed\":  [\"_unparsed\", \"raw\", \"original\"],\n",
      "    }\n",
      "\n",
      "    transformed = []\n",
      "    for src in parsed_dict:\n",
      "        out = {}\n",
      "        for target, candidates in field_map.items():\n",
      "            value = None\n",
      "            for key in candidates:\n",
      "                if key in src and src[key] is not None:\n",
      "                    value = src[key]\n",
      "                    break\n",
      "            # Apply type inference for optional fields; user_id is always cast to str\n",
      "            if target == \"user_id\":\n",
      "                # Ensure user_id is a string (as required by the schema)\n",
      "                out[target] = str(_infer_type(value)) if value is not None else None\n",
      "            else:\n",
      "                out[target] = _infer_type(value) if value is not None else None\n",
      "        transformed.append(out)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:07\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:135\u001b[0m [Code Tester] Stress-testing parser: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': '123', 'user_name': None, 'level': None, 'unparsed': '2026-01-30 14:22:01 INFO User login successful'} -- PASSED\n",
      "--> Exported: parser_LogEntry_struct_3.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:07\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3536762348.py:26\u001b[0m --- Finished Cluster 3 ---\n",
      "\u001b[38;5;245m2026-02-02 00:15:07\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:80\u001b[0m [Scehma Tester] Validating Functional BaseModel: 0\n",
      "\u001b[38;5;245m2026-02-02 00:15:07\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:99\u001b[0m [Coder] Parser for Gold Schema: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries\n",
      "    that match the ``LogEntry`` target schema.\n",
      "\n",
      "    The function:\n",
      "    * Coalesces possible source keys for each target field.\n",
      "    * Attempts to infer a more specific Python type from string values\n",
      "      (int, float, bool) while leaving other values untouched.\n",
      "    * Returns a list of plain ``dict`` objects ready to be fed to the\n",
      "      ``LogEntry`` Pydantic model.\n",
      "    \"\"\"\n",
      "\n",
      "    def _coerce(value):\n",
      "        \"\"\"Best‑effort conversion of a string to int, float or bool.\"\"\"\n",
      "        if not isinstance(value, str):\n",
      "            return value\n",
      "        v = value.strip()\n",
      "        # Boolean detection\n",
      "        low = v.lower()\n",
      "        if low in {\"true\", \"false\"}:\n",
      "            return low == \"true\"\n",
      "        # Integer detection\n",
      "        if v.isdigit() or (v.startswith(\"-\") and v[1:].isdigit()):\n",
      "            try:\n",
      "                return int(v)\n",
      "            except ValueError:\n",
      "                pass\n",
      "        # Float detection (including currency like \"$120.00\")\n",
      "        try:\n",
      "            # Remove common non‑numeric prefixes/suffixes\n",
      "            cleaned = v.lstrip(\"$€£¥\").replace(\",\", \"\")\n",
      "            return float(cleaned)\n",
      "        except ValueError:\n",
      "            return v  # fallback to original string\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "    for src in parsed_dict:\n",
      "        # Coalesce possible source keys\n",
      "        user_id = src.get(\"user_id\") or src.get(\"id\")\n",
      "        user_name = src.get(\"user\") or src.get(\"name\")\n",
      "        level = src.get(\"level\")\n",
      "        unparsed = src.get(\"unparsed\") or src.get(\"raw\") or src.get(\"original\")\n",
      "\n",
      "        # Apply type inference where appropriate\n",
      "        user_id = _coerce(user_id) if user_id is not None else None\n",
      "        user_name = _coerce(user_name) if user_name is not None else None\n",
      "        level = _coerce(level) if level is not None else None\n",
      "        unparsed = _coerce(unparsed) if unparsed is not None else None\n",
      "\n",
      "        transformed.append(\n",
      "            {\n",
      "                \"user_id\": user_id,\n",
      "                \"user_name\": user_name,\n",
      "                \"level\": level,\n",
      "                \"unparsed\": unparsed,\n",
      "            }\n",
      "        )\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:09\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:135\u001b[0m [Code Tester] Stress-testing parser: 1\n",
      "\u001b[38;5;245m2026-02-02 00:15:09\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:99\u001b[0m [Coder] Parser for Gold Schema: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': 1, 'user_name': 'Sam', 'level': 'INFO', 'unparsed': None} -- FAILED\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries that\n",
      "    conform to the ``LogEntry`` target schema.\n",
      "\n",
      "    The function follows a *coalesce* strategy:\n",
      "      * ``user_id``  ← first non‑null of ``'user_id'`` then ``'id'`` (required)\n",
      "      * ``user_name``← first non‑null of ``'user_name'`` then ``'user'``\n",
      "      * ``level``    ← ``'level'`` (optional)\n",
      "      * ``unparsed`` ← first non‑null of ``'unparsed'`` then ``'raw'``\n",
      "\n",
      "    It also attempts to infer the most appropriate Python type for string values:\n",
      "      * integer strings → ``int``\n",
      "      * floating‑point strings → ``float``\n",
      "      * case‑insensitive ``'true'`` / ``'false'`` → ``bool``\n",
      "      * otherwise the original string is kept.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    parsed_dict: list[dict]\n",
      "        List of dictionaries produced by a prior, permissive parsing step.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    list[dict]\n",
      "        List of dictionaries ready to be fed into ``LogEntry(**item)``.\n",
      "    \"\"\"\n",
      "    def _infer_type(val):\n",
      "        \"\"\"Best‑effort conversion of a string to int, float, bool or leave as‑is.\"\"\"\n",
      "        if not isinstance(val, str):\n",
      "            return val\n",
      "        lowered = val.strip().lower()\n",
      "        # Boolean detection\n",
      "        if lowered == \"true\":\n",
      "            return True\n",
      "        if lowered == \"false\":\n",
      "            return False\n",
      "        # Integer detection\n",
      "        if lowered.isdigit() or (lowered.startswith('-') and lowered[1:].isdigit()):\n",
      "            try:\n",
      "                return int(lowered)\n",
      "            except ValueError:\n",
      "                pass\n",
      "        # Float detection\n",
      "        try:\n",
      "            # Handles cases like \"12.\", \".5\", \"1e-3\"\n",
      "            return float(lowered)\n",
      "        except ValueError:\n",
      "            pass\n",
      "        # Return original string if no conversion succeeded\n",
      "        return val\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "    for src in parsed_dict:\n",
      "        # Coalesce fields according to the specification\n",
      "        user_id_raw = src.get(\"user_id\") or src.get(\"id\")\n",
      "        user_name_raw = src.get(\"user_name\") or src.get(\"user\")\n",
      "        level_raw = src.get(\"level\")\n",
      "        unparsed_raw = src.get(\"unparsed\") or src.get(\"raw\")\n",
      "\n",
      "        # Apply type inference where appropriate\n",
      "        user_id = _infer_type(user_id_raw) if user_id_raw is not None else None\n",
      "        user_name = _infer_type(user_name_raw) if user_name_raw is not None else None\n",
      "        level = _infer_type(level_raw) if level_raw is not None else None\n",
      "        unparsed = _infer_type(unparsed_raw) if unparsed_raw is not None else None\n",
      "\n",
      "        # Assemble the target dict; ``user_id`` is required by the schema,\n",
      "        # but we still include it even if None – downstream validation will handle it.\n",
      "        transformed.append(\n",
      "            {\n",
      "                \"user_id\": user_id,\n",
      "                \"user_name\": user_name,\n",
      "                \"level\": level,\n",
      "                \"unparsed\": unparsed,\n",
      "            }\n",
      "        )\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:12\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:135\u001b[0m [Code Tester] Stress-testing parser: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': 1, 'user_name': 'Sam', 'level': 'INFO', 'unparsed': None} -- FAILED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:12\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:99\u001b[0m [Coder] Parser for Gold Schema: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries\n",
      "    that match the ``LogEntry`` schema.\n",
      "\n",
      "    The function:\n",
      "    * Coalesces possible source keys for each target field.\n",
      "    * Attempts to infer a more specific Python type from string values\n",
      "      (int, float, bool) while leaving other values untouched.\n",
      "    * Returns plain ``dict`` objects – suitable for feeding into the\n",
      "      ``LogEntry`` Pydantic model elsewhere.\n",
      "    \"\"\"\n",
      "\n",
      "    def _coerce(value):\n",
      "        \"\"\"Best‑effort conversion of a string to int, float or bool.\"\"\"\n",
      "        if not isinstance(value, str):\n",
      "            return value\n",
      "        v = value.strip()\n",
      "        # Boolean detection\n",
      "        low = v.lower()\n",
      "        if low in {\"true\", \"false\"}:\n",
      "            return low == \"true\"\n",
      "        # Integer detection\n",
      "        if v.isdigit() or (v.startswith(\"-\") and v[1:].isdigit()):\n",
      "            try:\n",
      "                return int(v)\n",
      "            except ValueError:\n",
      "                pass\n",
      "        # Float detection (including currency like \"$120.00\")\n",
      "        try:\n",
      "            # Remove common non‑numeric prefixes/suffixes\n",
      "            cleaned = v.replace(\"$\", \"\").replace(\",\", \"\")\n",
      "            return float(cleaned)\n",
      "        except ValueError:\n",
      "            return v  # fallback to original string\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "    for src in parsed_dict:\n",
      "        # Coalesce possible source keys\n",
      "        user_id = src.get(\"user_id\") or src.get(\"id\")\n",
      "        user_name = src.get(\"user\") or src.get(\"name\")\n",
      "        level = src.get(\"level\")\n",
      "        unparsed = src.get(\"unparsed\") or src.get(\"raw\") or src.get(\"original\")\n",
      "\n",
      "        # Apply type inference where appropriate\n",
      "        user_id = _coerce(user_id) if user_id is not None else None\n",
      "        user_name = _coerce(user_name) if user_name is not None else None\n",
      "        level = _coerce(level) if level is not None else None\n",
      "        unparsed = _coerce(unparsed) if unparsed is not None else None\n",
      "\n",
      "        transformed.append(\n",
      "            {\n",
      "                \"user_id\": user_id,\n",
      "                \"user_name\": user_name,\n",
      "                \"level\": level,\n",
      "                \"unparsed\": unparsed,\n",
      "            }\n",
      "        )\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:15\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:135\u001b[0m [Code Tester] Stress-testing parser: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': 1, 'user_name': 'Sam', 'level': 'INFO', 'unparsed': None} -- FAILED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:15\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:99\u001b[0m [Coder] Parser for Gold Schema: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries that\n",
      "    conform to the ``LogEntry`` schema.\n",
      "\n",
      "    The function:\n",
      "    * Coalesces possible source keys for each target field.\n",
      "    * Uses ``dict.get`` for optional fields.\n",
      "    * Infers a more appropriate Python type from string values\n",
      "      (int → float → bool → original string).\n",
      "    \"\"\"\n",
      "\n",
      "    def _coalesce(keys: list[str], src: dict):\n",
      "        \"\"\"Return the first non‑None value for any of the supplied keys.\"\"\"\n",
      "        for k in keys:\n",
      "            if (v := src.get(k)) is not None:\n",
      "                return v\n",
      "        return None\n",
      "\n",
      "    def _infer_type(val):\n",
      "        \"\"\"Best‑effort conversion of a string to int, float, or bool.\"\"\"\n",
      "        if isinstance(val, str):\n",
      "            low = val.strip().lower()\n",
      "            # Boolean\n",
      "            if low in {\"true\", \"false\"}:\n",
      "                return low == \"true\"\n",
      "            # Integer\n",
      "            try:\n",
      "                return int(val)\n",
      "            except ValueError:\n",
      "                pass\n",
      "            # Float (including currency like $120.00)\n",
      "            try:\n",
      "                # Strip common non‑numeric prefixes/suffixes\n",
      "                cleaned = val.replace(\"$\", \"\").replace(\",\", \"\")\n",
      "                return float(cleaned)\n",
      "            except ValueError:\n",
      "                pass\n",
      "        return val\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "    for src in parsed_dict:\n",
      "        entry = {\n",
      "            # user_id: prefer explicit 'user_id', fall back to generic 'id'\n",
      "            \"user_id\": _infer_type(_coalesce([\"user_id\", \"id\"], src)),\n",
      "            # user_name: common alternatives\n",
      "            \"user_name\": _infer_type(_coalesce([\"user_name\", \"user\", \"name\"], src)),\n",
      "            # level: direct mapping, optional\n",
      "            \"level\": _infer_type(src.get(\"level\")),\n",
      "            # unparsed: keep raw line if present under typical keys\n",
      "            \"unparsed\": _infer_type(_coalesce([\"unparsed\", \"raw\", \"line\"], src)),\n",
      "        }\n",
      "\n",
      "        # Remove keys that are still None (optional fields) to let Pydantic apply defaults\n",
      "        entry = {k: v for k, v in entry.items() if v is not None}\n",
      "        transformed.append(entry)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:17\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:135\u001b[0m [Code Tester] Stress-testing parser: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': 1, 'user_name': 'Sam', 'level': 'INFO'} -- FAILED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:17\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:99\u001b[0m [Coder] Parser for Gold Schema: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries\n",
      "    that match the ``LogEntry`` target schema.\n",
      "\n",
      "    The function:\n",
      "    * Coalesces possible source keys for each target field.\n",
      "    * Attempts to infer a more specific Python type from string values\n",
      "      (int, float, bool) while leaving other values untouched.\n",
      "    * Returns a list of plain ``dict`` objects ready for Pydantic validation.\n",
      "    \"\"\"\n",
      "\n",
      "    def _coerce(value):\n",
      "        \"\"\"Best‑effort conversion of a string to int, float or bool.\"\"\"\n",
      "        if not isinstance(value, str):\n",
      "            return value\n",
      "        v = value.strip()\n",
      "        # Boolean detection\n",
      "        low = v.lower()\n",
      "        if low in {\"true\", \"false\"}:\n",
      "            return low == \"true\"\n",
      "        # Integer detection\n",
      "        if v.isdigit() or (v.startswith(\"-\") and v[1:].isdigit()):\n",
      "            try:\n",
      "                return int(v)\n",
      "            except ValueError:\n",
      "                pass\n",
      "        # Float detection (including currency symbols)\n",
      "        try:\n",
      "            # Remove common non‑numeric prefixes/suffixes like \"$\" or \"%\"\n",
      "            cleaned = v.lstrip(\"$€£¥\").rstrip(\"%\")\n",
      "            return float(cleaned)\n",
      "        except ValueError:\n",
      "            pass\n",
      "        return v\n",
      "\n",
      "    transformed = []\n",
      "    for src in parsed_dict:\n",
      "        # Coalesce possible source keys\n",
      "        user_id_raw = src.get(\"user_id\") or src.get(\"id\")\n",
      "        user_name_raw = src.get(\"user\") or src.get(\"name\")\n",
      "        level_raw = src.get(\"level\")\n",
      "        unparsed_raw = src.get(\"unparsed\") or src.get(\"raw\") or src.get(\"original\")\n",
      "\n",
      "        # Apply type inference where appropriate\n",
      "        entry = {\n",
      "            \"user_id\": _coerce(user_id_raw) if user_id_raw is not None else None,\n",
      "            \"user_name\": _coerce(user_name_raw) if user_name_raw is not None else None,\n",
      "            \"level\": _coerce(level_raw) if level_raw is not None else None,\n",
      "            \"unparsed\": _coerce(unparsed_raw) if unparsed_raw is not None else None,\n",
      "        }\n",
      "        transformed.append(entry)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:19\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:135\u001b[0m [Code Tester] Stress-testing parser: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': 1, 'user_name': 'Sam', 'level': 'INFO', 'unparsed': None} -- FAILED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:19\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:99\u001b[0m [Coder] Parser for Gold Schema: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries that\n",
      "    match the ``LogEntry`` target schema.\n",
      "\n",
      "    The function:\n",
      "    * Coalesces possible source keys for each target field.\n",
      "    * Uses ``dict.get`` for optional look‑ups.\n",
      "    * Infers a more appropriate Python type for string values\n",
      "      (bool, int, float) while leaving non‑string values untouched.\n",
      "    * Returns a list of plain ``dict`` objects ready to be fed to the\n",
      "      ``LogEntry`` Pydantic model.\n",
      "    \"\"\"\n",
      "\n",
      "    def _infer_type(value):\n",
      "        \"\"\"Best‑effort conversion of a string to bool, int or float.\"\"\"\n",
      "        if not isinstance(value, str):\n",
      "            return value\n",
      "\n",
      "        low = value.strip().lower()\n",
      "        # Boolean detection\n",
      "        if low == \"true\":\n",
      "            return True\n",
      "        if low == \"false\":\n",
      "            return False\n",
      "\n",
      "        # Integer detection (handles optional leading sign)\n",
      "        if low.lstrip(\"-+\").isdigit():\n",
      "            try:\n",
      "                return int(low)\n",
      "            except ValueError:\n",
      "                pass\n",
      "\n",
      "        # Float detection – strip common currency symbols and commas first\n",
      "        cleaned = low.replace(\"$\", \"\").replace(\",\", \"\")\n",
      "        try:\n",
      "            return float(cleaned)\n",
      "        except ValueError:\n",
      "            return value  # fall back to original string\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "\n",
      "    for src in parsed_dict:\n",
      "        # --- coalesce source keys -------------------------------------------------\n",
      "        user_id = src.get(\"user_id\") or src.get(\"id\")\n",
      "        user_name = src.get(\"user_name\") or src.get(\"user\")\n",
      "        level = src.get(\"level\")\n",
      "        unparsed = src.get(\"unparsed\")\n",
      "\n",
      "        # --- type inference for optional fields (skip user_id which must stay str) ---\n",
      "        user_name = _infer_type(user_name) if user_name is not None else None\n",
      "        level = _infer_type(level) if level is not None else None\n",
      "        unparsed = _infer_type(unparsed) if unparsed is not None else None\n",
      "\n",
      "        # Ensure user_id is a string (as required by the schema)\n",
      "        user_id = str(user_id) if user_id is not None else None\n",
      "\n",
      "        transformed.append(\n",
      "            {\n",
      "                \"user_id\": user_id,\n",
      "                \"user_name\": user_name,\n",
      "                \"level\": level,\n",
      "                \"unparsed\": unparsed,\n",
      "            }\n",
      "        )\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:22\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:135\u001b[0m [Code Tester] Stress-testing parser: 6\n",
      "\u001b[38;5;245m2026-02-02 00:15:22\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3536762348.py:26\u001b[0m --- Finished Cluster 2 ---\n",
      "\u001b[38;5;245m2026-02-02 00:15:22\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:42\u001b[0m [Architect] Defining Semantic Goal: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': '1', 'user_name': 'Sam', 'level': 'INFO', 'unparsed': None} -- PASSED\n",
      "--> Exported: parser_LogEntry_struct_2.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:30\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:80\u001b[0m [Scehma Tester] Validating Functional BaseModel: 1\n",
      "\u001b[38;5;245m2026-02-02 00:15:30\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:99\u001b[0m [Coder] Parser for Gold Schema: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List, Dict, Optional\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "class LogRecord(BaseModel):\n",
      "    timestamp: Optional[str] = Field(\n",
      "        default=None,\n",
      "        description=\"ISO 8601 timestamp of the event, e.g., '2026-01-30T14:22:01Z'.\"\n",
      "    )\n",
      "    level: Optional[str] = Field(\n",
      "        default=None,\n",
      "        description=\"Log severity level such as 'INFO', 'DEBUG', etc.\"\n",
      "    )\n",
      "    user: Optional[str] = Field(\n",
      "        default=None,\n",
      "        description=\"Identifier of the user associated with the event.\"\n",
      "    )\n",
      "    action: Optional[str] = Field(\n",
      "        default=None,\n",
      "        description=\"Action performed by the user, e.g., 'login'.\"\n",
      "    )\n",
      "    success: bool = Field(\n",
      "        default=False,\n",
      "        description=\"Indicates whether the action was successful.\"\n",
      "    )\n",
      "    cpu_usage: Optional[float] = Field(\n",
      "        default=None,\n",
      "        description=\"CPU usage percentage, represented as a float. Commas are ignored.\"\n",
      "    )\n",
      "    memory: Optional[str] = Field(\n",
      "        default=None,\n",
      "        description=\"Memory usage string, including unit, e.g., '512MB'.\"\n",
      "    )\n",
      "    service: Optional[str] = Field(\n",
      "        default=None,\n",
      "        description=\"Name of the service generating the log, e.g., 'orders'.\"\n",
      "    )\n",
      "    order_id: Optional[int] = Field(\n",
      "        default=None,\n",
      "        description=\"Identifier of the order related to the event.\"\n",
      "    )\n",
      "    status: Optional[str] = Field(\n",
      "        default=None,\n",
      "        description=\"Current status of the order, e.g., 'created'.\"\n",
      "    )\n",
      "    unparsed: Optional[str] = Field(\n",
      "        default=None,\n",
      "        description=\"Raw log line when the structured fields could not be extracted.\"\n",
      "    )\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Convert a list of loosely‑parsed log dictionaries into a list of dictionaries\n",
      "    that conform to the LogRecord schema.\n",
      "\n",
      "    The function:\n",
      "    * coalesces multiple possible source keys for each target field,\n",
      "    * infers and casts simple types (bool, int, float) from string values,\n",
      "    * strips commas from numeric strings,\n",
      "    * applies the default values defined in the target schema when a field is missing.\n",
      "    \"\"\"\n",
      "    # Mapping of target field -> possible source keys (ordered by priority)\n",
      "    COALESCE_MAP: dict[str, list[str]] = {\n",
      "        \"timestamp\": [\"timestamp\", \"time\", \"date\", \"datetime\"],\n",
      "        \"level\": [\"level\", \"severity\", \"log_level\"],\n",
      "        \"user\": [\"user\", \"username\", \"uid\"],\n",
      "        \"action\": [\"action\", \"event\", \"operation\"],\n",
      "        \"success\": [\"success\", \"result\", \"outcome\", \"status_ok\"],\n",
      "        \"cpu_usage\": [\"cpu_usage\", \"cpu\", \"cpu%\"],\n",
      "        \"memory\": [\"memory\", \"mem\", \"memory_usage\"],\n",
      "        \"service\": [\"service\", \"svc\", \"application\"],\n",
      "        \"order_id\": [\"order_id\", \"order\", \"orderId\", \"id\"],\n",
      "        \"status\": [\"status\", \"state\", \"order_status\"],\n",
      "        \"unparsed\": [\"unparsed\", \"raw\", \"raw_line\", \"message\"],\n",
      "    }\n",
      "\n",
      "    def _coalesce(src: dict, keys: list[str]) -> object:\n",
      "        \"\"\"Return the first non‑missing value for the given keys.\"\"\"\n",
      "        for k in keys:\n",
      "            if k in src:\n",
      "                return src[k]\n",
      "        return None\n",
      "\n",
      "    def _to_bool(val: object) -> bool:\n",
      "        if isinstance(val, bool):\n",
      "            return val\n",
      "        if isinstance(val, (int, float)):\n",
      "            return bool(val)\n",
      "        if isinstance(val, str):\n",
      "            lowered = val.strip().lower()\n",
      "            if lowered in {\"true\", \"t\", \"yes\", \"y\", \"1\"}:\n",
      "                return True\n",
      "            if lowered in {\"false\", \"f\", \"no\", \"n\", \"0\"}:\n",
      "                return False\n",
      "        # fallback – treat any truthy value as True\n",
      "        return bool(val)\n",
      "\n",
      "    def _to_int(val: object) -> int | None:\n",
      "        if isinstance(val, int):\n",
      "            return val\n",
      "        if isinstance(val, float):\n",
      "            return int(val)\n",
      "        if isinstance(val, str):\n",
      "            try:\n",
      "                # remove commas and possible surrounding whitespace\n",
      "                return int(val.replace(\",\", \"\").strip())\n",
      "            except ValueError:\n",
      "                return None\n",
      "        return None\n",
      "\n",
      "    def _to_float(val: object) -> float | None:\n",
      "        if isinstance(val, (int, float)):\n",
      "            return float(val)\n",
      "        if isinstance(val, str):\n",
      "            try:\n",
      "                # commas are ignored per spec\n",
      "                return float(val.replace(\",\", \"\").strip())\n",
      "            except ValueError:\n",
      "                return None\n",
      "        return None\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "\n",
      "    for src in parsed_dict:\n",
      "        out: dict = {}\n",
      "\n",
      "        # ----- simple string fields (keep as‑is) -----\n",
      "        for field in (\"timestamp\", \"level\", \"user\", \"action\", \"memory\", \"service\", \"status\", \"unparsed\"):\n",
      "            raw = _coalesce(src, COALESCE_MAP[field])\n",
      "            out[field] = raw if raw is not None else None\n",
      "\n",
      "        # ----- boolean field -----\n",
      "        raw_success = _coalesce(src, COALESCE_MAP[\"success\"])\n",
      "        out[\"success\"] = _to_bool(raw_success) if raw_success is not None else False\n",
      "\n",
      "        # ----- numeric fields -----\n",
      "        raw_cpu = _coalesce(src, COALESCE_MAP[\"cpu_usage\"])\n",
      "        out[\"cpu_usage\"] = _to_float(raw_cpu) if raw_cpu is not None else None\n",
      "\n",
      "        raw_order = _coalesce(src, COALESCE_MAP[\"order_id\"])\n",
      "        out[\"order_id\"] = _to_int(raw_order) if raw_order is not None else None\n",
      "\n",
      "        transformed.append(out)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:32\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:135\u001b[0m [Code Tester] Stress-testing parser: 2\n",
      "\u001b[38;5;245m2026-02-02 00:15:32\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3536762348.py:26\u001b[0m --- Finished Cluster 2 ---\n",
      "\u001b[38;5;245m2026-02-02 00:15:32\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:80\u001b[0m [Scehma Tester] Validating Functional BaseModel: 0\n",
      "\u001b[38;5;245m2026-02-02 00:15:32\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:99\u001b[0m [Coder] Parser for Gold Schema: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'timestamp': '2026-01-30T14:22:01Z', 'level': 'INFO', 'user': 'alice', 'action': 'login', 'memory': None, 'service': None, 'status': None, 'unparsed': None, 'success': True, 'cpu_usage': None, 'order_id': None} -- PASSED\n",
      "Input: {'timestamp': None, 'level': 'INFO', 'user': None, 'action': None, 'memory': '512MB', 'service': None, 'status': None, 'unparsed': None, 'success': False, 'cpu_usage': 1234.56, 'order_id': None} -- PASSED\n",
      "--> Exported: parser_LogRecord_struct_2.py\n",
      "```python\n",
      "import re\n",
      "from typing import List, Dict, Any, Optional\n",
      "\n",
      "def transform_to_models(parsed_dict: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries that\n",
      "    conform to the LogRecord schema.\n",
      "\n",
      "    The function follows a *coalesce* strategy: for each target field it checks a\n",
      "    collection of possible source keys, then falls back to regex extraction from the\n",
      "    raw `_unparsed` string when available.\n",
      "    \"\"\"\n",
      "\n",
      "    # ---------- helper utilities ----------\n",
      "    _ts_regex = re.compile(\n",
      "        r'(?P<ts>\\d{4}-\\d{2}-\\d{2}[ T]\\d{2}:\\d{2}:\\d{2}(?:[.,]\\d+)?(?:Z|[+-]\\d{2}:?\\d{2})?)'\n",
      "    )\n",
      "    _level_regex = re.compile(r'\\b(INFO|DEBUG|WARN|ERROR|TRACE|FATAL)\\b', re.IGNORECASE)\n",
      "    _user_regex = re.compile(r'\\bUser\\s+([A-Za-z0-9_-]+)', re.IGNORECASE)\n",
      "    _action_regex = re.compile(r'\\b(login|logout|create|delete|update|run|started|stopped|health\\s+check|cron\\s+job)\\b', re.IGNORECASE)\n",
      "    _success_regex = re.compile(r'\\b(successful|success|failed|failure|error)\\b', re.IGNORECASE)\n",
      "    _cpu_regex = re.compile(r'CPU[:\\s]*([\\d,]+(?:\\.\\d+)?)\\s*%?', re.IGNORECASE)\n",
      "    _mem_regex = re.compile(r'(\\d+(?:\\.\\d+)?\\s*(?:KB|MB|GB|TB))', re.IGNORECASE)\n",
      "    _order_regex = re.compile(r'order[_\\s]?id[:\\s]*([0-9]+)', re.IGNORECASE)\n",
      "\n",
      "    def _coalesce(src: Dict[str, Any], keys: List[str]) -> Optional[Any]:\n",
      "        \"\"\"Return the first non‑None value found for the supplied keys.\"\"\"\n",
      "        for k in keys:\n",
      "            if k in src and src[k] is not None:\n",
      "                return src[k]\n",
      "        return None\n",
      "\n",
      "    def _to_bool(val: Any) -> bool:\n",
      "        if isinstance(val, bool):\n",
      "            return val\n",
      "        if isinstance(val, (int, float)):\n",
      "            return bool(val)\n",
      "        if isinstance(val, str):\n",
      "            lowered = val.strip().lower()\n",
      "            if lowered in {\"true\", \"yes\", \"1\", \"successful\", \"success\"}:\n",
      "                return True\n",
      "            if lowered in {\"false\", \"no\", \"0\", \"failed\", \"failure\", \"error\"}:\n",
      "                return False\n",
      "        return False\n",
      "\n",
      "    def _to_float(val: Any) -> Optional[float]:\n",
      "        if isinstance(val, (int, float)):\n",
      "            return float(val)\n",
      "        if isinstance(val, str):\n",
      "            # remove commas used as thousand separators\n",
      "            cleaned = val.replace(\",\", \"\")\n",
      "            try:\n",
      "                return float(cleaned)\n",
      "            except ValueError:\n",
      "                return None\n",
      "        return None\n",
      "\n",
      "    def _to_int(val: Any) -> Optional[int]:\n",
      "        if isinstance(val, int):\n",
      "            return val\n",
      "        if isinstance(val, float):\n",
      "            return int(val)\n",
      "        if isinstance(val, str):\n",
      "            cleaned = val.replace(\",\", \"\")\n",
      "            try:\n",
      "                return int(cleaned)\n",
      "            except ValueError:\n",
      "                return None\n",
      "        return None\n",
      "\n",
      "    # ---------- main transformation ----------\n",
      "    transformed: List[Dict[str, Any]] = []\n",
      "\n",
      "    for src in parsed_dict:\n",
      "        # start with defaults\n",
      "        out: Dict[str, Any] = {\n",
      "            \"timestamp\": None,\n",
      "            \"level\": None,\n",
      "            \"user\": None,\n",
      "            \"action\": None,\n",
      "            \"success\": False,\n",
      "            \"cpu_usage\": None,\n",
      "            \"memory\": None,\n",
      "            \"service\": None,\n",
      "            \"order_id\": None,\n",
      "            \"status\": None,\n",
      "            \"unparsed\": None,\n",
      "        }\n",
      "\n",
      "        # raw line (if present)\n",
      "        raw_line: Optional[str] = src.get(\"_unparsed\")\n",
      "        if raw_line:\n",
      "            out[\"unparsed\"] = raw_line\n",
      "\n",
      "        # ----- direct coalesce from known keys -----\n",
      "        out[\"timestamp\"] = _coalesce(src, [\"timestamp\", \"time\", \"date\"])\n",
      "        out[\"level\"] = _coalesce(src, [\"level\", \"severity\"])\n",
      "        out[\"user\"] = _coalesce(src, [\"user\", \"username\", \"uid\"])\n",
      "        out[\"action\"] = _coalesce(src, [\"action\", \"event\"])\n",
      "        out[\"success\"] = _to_bool(_coalesce(src, [\"success\", \"result\", \"outcome\"]))\n",
      "        out[\"cpu_usage\"] = _to_float(_coalesce(src, [\"cpu_usage\", \"cpu\"]))\n",
      "        out[\"memory\"] = _coalesce(src, [\"memory\", \"mem\"])\n",
      "        out[\"service\"] = _coalesce(src, [\"service\", \"svc\"])\n",
      "        out[\"order_id\"] = _to_int(_coalesce(src, [\"order_id\", \"orderId\", \"order\"]))\n",
      "        out[\"status\"] = _coalesce(src, [\"status\", \"state\"])\n",
      "\n",
      "        # ----- fallback extraction from the raw line -----\n",
      "        if raw_line:\n",
      "            # timestamp\n",
      "            if not out[\"timestamp\"]:\n",
      "                m = _ts_regex.search(raw_line)\n",
      "                if m:\n",
      "                    out[\"timestamp\"] = m.group(\"ts\")\n",
      "\n",
      "            # level\n",
      "            if not out[\"level\"]:\n",
      "                m = _level_regex.search(raw_line)\n",
      "                if m:\n",
      "                    out[\"level\"] = m.group(1).upper()\n",
      "\n",
      "            # user\n",
      "            if not out[\"user\"]:\n",
      "                m = _user_regex.search(raw_line)\n",
      "                if m:\n",
      "                    out[\"user\"] = m.group(1)\n",
      "\n",
      "            # action\n",
      "            if not out[\"action\"]:\n",
      "                m = _action_regex.search(raw_line)\n",
      "                if m:\n",
      "                    out[\"action\"] = m.group(1).lower()\n",
      "\n",
      "            # success flag\n",
      "            if not out[\"success\"]:\n",
      "                m = _success_regex.search(raw_line)\n",
      "                if m:\n",
      "                    out[\"success\"] = _to_bool(m.group(1))\n",
      "\n",
      "            # cpu usage\n",
      "            if out[\"cpu_usage\"] is None:\n",
      "                m = _cpu_regex.search(raw_line)\n",
      "                if m:\n",
      "                    out[\"cpu_usage\"] = _to_float(m.group(1))\n",
      "\n",
      "            # memory\n",
      "            if not out[\"memory\"]:\n",
      "                m = _mem_regex.search(raw_line)\n",
      "                if m:\n",
      "                    out[\"memory\"] = m.group(1)\n",
      "\n",
      "            # order_id\n",
      "            if out[\"order_id\"] is None:\n",
      "                m = _order_regex.search(raw_line)\n",
      "                if m:\n",
      "                    out[\"order_id\"] = _to_int(m.group(1))\n",
      "\n",
      "        # Ensure proper types for fields that may still be strings\n",
      "        if isinstance(out[\"timestamp\"], (int, float)):\n",
      "            out[\"timestamp\"] = str(out[\"timestamp\"])\n",
      "\n",
      "        # Append the cleaned record\n",
      "        transformed.append(out)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:38\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3328042808.py:135\u001b[0m [Code Tester] Stress-testing parser: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'timestamp': '2026-01-30 14:22:01', 'level': 'INFO', 'user': 'login', 'action': 'login', 'success': True, 'cpu_usage': None, 'memory': None, 'service': None, 'order_id': None, 'status': None, 'unparsed': '2026-01-30 14:22:01 INFO User login successful'} -- PASSED\n",
      "Input: {'timestamp': '2026-01-31 17:11:22', 'level': 'INFO', 'user': None, 'action': None, 'success': False, 'cpu_usage': None, 'memory': None, 'service': None, 'order_id': None, 'status': None, 'unparsed': '[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"'} -- PASSED\n",
      "Input: {'timestamp': '2026-01-31 17:11:00', 'level': 'DEBUG', 'user': None, 'action': 'cron job', 'success': False, 'cpu_usage': None, 'memory': None, 'service': None, 'order_id': None, 'status': None, 'unparsed': '2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check'} -- PASSED\n",
      "Input: {'timestamp': None, 'level': 'INFO', 'user': None, 'action': None, 'success': False, 'cpu_usage': None, 'memory': None, 'service': 'orders', 'order_id': 1001, 'status': 'created', 'unparsed': None} -- PASSED\n",
      "--> Exported: parser_LogRecord_struct_3.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 00:15:38\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m3536762348.py:26\u001b[0m --- Finished Cluster 3 ---\n"
     ]
    }
   ],
   "source": [
    "async def run_data_matrix(all_data: Dict[int, List[Dict]]):\n",
    "    for sem_id, records in all_data.items():\n",
    "        struct_groups = {}\n",
    "        for s in records:\n",
    "            cid = s['structure_cluster_id']\n",
    "            struct_groups.setdefault(cid, []).append(s)\n",
    "\n",
    "        shared_gold_schema = None\n",
    "        \n",
    "        for struct_id, pairs in struct_groups.items():\n",
    "            config = {\"configurable\": {\"thread_id\": f\"sem_{sem_id}_str_{struct_id}\"}}\n",
    "            \n",
    "            # Fix the typo 'structure_causter_id' -> 'structure_cluster_id'\n",
    "            initial_state = {\n",
    "                \"semantic_id\": str(sem_id),\n",
    "                \"structure_cluster_id\": str(struct_id),\n",
    "                \"data_pairs_all\": records,\n",
    "                \"data_pairs_structure\": pairs,\n",
    "                \"gold_schema\": shared_gold_schema,\n",
    "                \"attempts\": 0\n",
    "            }\n",
    "            \n",
    "            final_output = await app.ainvoke(initial_state, config)\n",
    "            shared_gold_schema = final_output.get('gold_schema')\n",
    "            \n",
    "            logger.info(f\"--- Finished Cluster {struct_id} ---\")\n",
    "\n",
    "await run_data_matrix(cluster_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d36ef94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dq-swirl (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
